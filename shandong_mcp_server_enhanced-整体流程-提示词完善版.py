#!/usr/bin/env python3
"""
å±±ä¸œè€•åœ°æµå‡ºåˆ†æ MCPæœåŠ¡å™¨ - å¢å¼ºç‰ˆ
æ•´åˆFastMCPæ¡†æ¶ï¼Œæ”¯æŒHTTPå’Œstdioä¸¤ç§ä¼ è¾“æ–¹å¼
"""

import asyncio
import json
import logging
import httpx
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, TypeVar
from pydantic import BaseModel
from enum import IntEnum

# MCP SDK å¯¼å…¥
try:
    from mcp.server.fastmcp import FastMCP, Context
    from mcp.server.sse import SseServerTransport
    from mcp.server import Server
    from starlette.applications import Starlette
    from starlette.requests import Request
    from starlette.responses import JSONResponse
    from starlette.routing import Mount, Route
    import uvicorn
    import argparse
except ImportError as e:
    print(f"Error importing enhanced MCP dependencies: {e}")
    print("Please install: pip install fastmcp starlette uvicorn")
    exit(1)

T = TypeVar("T")

# ============ é…ç½®éƒ¨åˆ† - é¥æ„Ÿå¤§æ¥¼ç¯å¢ƒé€‚é… ============

# æœåŠ¡å™¨é…ç½®
MCP_SERVER_NAME = "yaogan-building-cultivated-analysis"

# é¥æ„Ÿå¤§æ¥¼ç¯å¢ƒé…ç½®
YAOGAN_NETWORK_SEGMENT = "10.101.240"
OGE_FRONTEND_URL = "http://10.101.240.20"  # å‰ç«¯åœ°å€
OGE_BACKEND_URL = "http://10.101.240.20"   # åç«¯åœ°å€ï¼ˆé€šè¿‡å‰ç«¯ä»£ç†ï¼‰
COMPUTE_CLUSTER_MASTER = "http://10.101.240.10"  # è®¡ç®—é›†ç¾¤ä¸»èŠ‚ç‚¹(ogecal0)

# APIé…ç½® - é€‚é…é¥æ„Ÿå¤§æ¥¼ç¯å¢ƒ
OGE_API_BASE_URL = f"{OGE_BACKEND_URL}/gateway/computation-api/process"
INTRANET_API_BASE_URL = f"{OGE_BACKEND_URL}/api/computation/process"
INTRANET_AUTH_TOKEN = "Bearer TOKEN_TO_BE_UPDATED"  # éœ€è¦æ ¹æ®æ–°ç¯å¢ƒè·å–

# DAGæ‰¹å¤„ç†APIé…ç½® - ä½¿ç”¨LivyæœåŠ¡
DAG_API_BASE_URL = f"{COMPUTE_CLUSTER_MASTER}:8998/api/oge-dag"
LIVY_API_BASE_URL = f"{COMPUTE_CLUSTER_MASTER}:8998"
DEFAULT_USER_ID = "yaogan-building-user"
DEFAULT_USERNAME = "oge_admin"

# MinIOå­˜å‚¨é…ç½®
MINIO_ENDPOINT = "http://10.101.240.23:9007"
MINIO_ACCESS_KEY = "oge"
MINIO_SECRET_KEY = "ypfamily608"

# ============ å“åº”æ ¼å¼å®šä¹‰ ============

class RetCode(IntEnum):
    SUCCESS = 0
    FAILED = 1

class Result(BaseModel):
    success: bool = False
    code: Optional[int] = None
    msg: Optional[str] = None
    data: Optional[T] = None
    operation: Optional[str] = None
    execution_time: Optional[float] = None
    api_endpoint: Optional[str] = "oge"

    @classmethod
    def succ(cls, data: T = None, msg="æˆåŠŸ", operation=None, execution_time=None, api_endpoint="oge"):
        return cls(
            success=True, 
            code=RetCode.SUCCESS, 
            msg=msg, 
            data=data,
            operation=operation,
            execution_time=execution_time,
            api_endpoint=api_endpoint
        )

    @classmethod
    def failed(cls, code: int = RetCode.FAILED, msg="æ“ä½œå¤±è´¥", operation=None):
        return cls(success=False, code=code, msg=msg, operation=operation)

# ============ æ—¥å¿—é…ç½® ============

def setup_logger(name: str = None, file: str = None, level=logging.INFO) -> logging.Logger:
    """è®¾ç½®ç»“æ„åŒ–æ—¥å¿—"""
    logger = logging.getLogger(name)
    logger.propagate = False
    logger.setLevel(level)
    
    if logger.hasHandlers():
        logger.handlers.clear()

    formatter = logging.Formatter(
        fmt='%(name)s - %(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # æ–‡ä»¶æ—¥å¿—
    if file:
        Path(file).parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(filename=file, mode='a', encoding='utf-8')
        file_handler.setFormatter(formatter)
        file_handler.setLevel(level)
        logger.addHandler(file_handler)

    # æ§åˆ¶å°æ—¥å¿—
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(formatter)
    stream_handler.setLevel(level)
    logger.addHandler(stream_handler)

    return logger

# åˆ›å»ºæ—¥å¿—å®ä¾‹
logger = setup_logger("yaogan_mcp", "logs/yaogan_mcp.log")
api_logger = setup_logger("yaogan_api", "logs/api_calls.log")

# ============ FastMCPå®ä¾‹ ============

mcp = FastMCP(MCP_SERVER_NAME)

# ============ Tokenç®¡ç† ============

async def refresh_intranet_token() -> tuple[bool, str]:
    """è‡ªåŠ¨åˆ·æ–°é¥æ„Ÿå¤§æ¥¼ç¯å¢ƒtoken"""
    global INTRANET_AUTH_TOKEN
    
    try:
        logger.info("å¼€å§‹åˆ·æ–°é¥æ„Ÿå¤§æ¥¼ç¯å¢ƒtoken...")
        
        url = f"{OGE_BACKEND_URL}/api/oauth/token"
        
        params = {
            "scopes": "web",
            "client_secret": "123456",
            "client_id": "oge_client",
            "grant_type": "password",
            "username": DEFAULT_USERNAME,
            "password": "123456"
        }
        
        body = {
            "username": DEFAULT_USERNAME,
            "password": "123456"
        }
        
        headers = {
            "Content-Type": "application/json"
        }
        
        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.post(url, params=params, json=body, headers=headers)
            
            if response.status_code == 200:
                data = response.json()
                
                if 'data' in data and 'token' in data['data']:
                    token = data['data']['token']
                    token_head = data['data'].get('tokenHead', 'Bearer').rstrip()  # å»æ‰å°¾éƒ¨ç©ºæ ¼
                    full_token = f"{token_head} {token}"
                    
                    # æ›´æ–°å…¨å±€token
                    INTRANET_AUTH_TOKEN = full_token
                    
                    logger.info(f"Tokenåˆ·æ–°æˆåŠŸ: {full_token[:50]}...")
                    logger.info(f"Tokenæ ¼å¼æ£€æŸ¥ - head: '{token_head}', length: {len(full_token)}")
                    return True, full_token
                else:
                    logger.error(f"Tokenå“åº”æ ¼å¼å¼‚å¸¸: {data}")
                    return False, f"Tokenå“åº”æ ¼å¼å¼‚å¸¸: {data}"
            else:
                error_msg = f"Tokenè·å–å¤±è´¥ - çŠ¶æ€ç : {response.status_code} - å“åº”: {response.text}"
                logger.error(error_msg)
                return False, error_msg
                
    except Exception as e:
        error_msg = f"Tokenåˆ·æ–°å¼‚å¸¸: {str(e)}"
        logger.error(error_msg)
        return False, error_msg

# ============ é€šç”¨APIè°ƒç”¨å‡½æ•° ============

async def call_api_with_timing(
    url: str,
    method: str = 'POST',
    json_data: dict = None,
    headers: dict = None,
    timeout: int = 120,
    auto_retry_on_token_expire: bool = True,
    use_intranet_token: bool = False
) -> tuple[dict, float]:
    """é€šç”¨APIè°ƒç”¨ï¼Œå¸¦æ€§èƒ½ç›‘æ§å’Œè‡ªåŠ¨tokenåˆ·æ–°"""
    global INTRANET_AUTH_TOKEN
    start_time = time.perf_counter()
    
    # å¦‚æœæŒ‡å®šä½¿ç”¨å†…ç½‘tokenï¼Œåˆ™åŠ¨æ€æ›´æ–°headers
    if use_intranet_token:
        if headers is None:
            headers = {"Content-Type": "application/json"}
        headers["Authorization"] = INTRANET_AUTH_TOKEN
        logger.info(f"ä½¿ç”¨å†…ç½‘token: {INTRANET_AUTH_TOKEN[:50]}...")
        logger.info(f"å®é™…å‘é€headers: {dict((k, v[:50] + '...' if k == 'Authorization' and len(v) > 50 else v) for k, v in headers.items())}")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨é‡è¯•
    should_auto_retry = (
        use_intranet_token and 
        auto_retry_on_token_expire
    )
    
    try:
        async with httpx.AsyncClient(timeout=timeout) as client:
            # å¤„ç†GETè¯·æ±‚çš„å‚æ•°
            if method.upper() == "GET" and headers and "params" in headers:
                params = headers.pop("params")
                response = await client.request(
                    method=method.upper(),
                    url=url,
                    params=params,
                    headers=headers or {"Content-Type": "application/json"}
                )
            else:
                response = await client.request(
                    method=method.upper(),
                    url=url,
                    json=json_data,
                    headers=headers or {"Content-Type": "application/json"}
                )
            
            execution_time = time.perf_counter() - start_time
            
            if response.status_code == 200:
                # å®‰å…¨å¤„ç†JSONè§£æ
                response_text = response.text.strip()
                try:
                    result = response.json()
                except Exception as json_error:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬ä½œä¸ºç»“æœ
                    logger.info(f"å“åº”ä¸æ˜¯JSONæ ¼å¼ï¼Œä½œä¸ºçº¯æ–‡æœ¬å¤„ç†: {response_text[:100]}...")
                    # å¯¹äºDAGçŠ¶æ€æŸ¥è¯¢ï¼Œç›´æ¥è¿”å›æ–‡æœ¬çŠ¶æ€
                    if "/getState" in url:
                        result = response_text if response_text else "unknown"
                    else:
                        result = {
                            "raw_text": response_text,
                            "json_parse_error": str(json_error),
                            "content_type": response.headers.get("content-type", "unknown")
                        }
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºtokenè¿‡æœŸé”™è¯¯
                if (should_auto_retry and 
                    isinstance(result, dict) and 
                    result.get("code") == 40003):
                    
                    logger.warning("æ£€æµ‹åˆ°tokenè¿‡æœŸ(40003)ï¼Œå°è¯•è‡ªåŠ¨åˆ·æ–°...")
                    
                    # åˆ·æ–°token
                    success, new_token = await refresh_intranet_token()
                    
                    if success:
                        logger.info("Tokenåˆ·æ–°æˆåŠŸï¼Œé‡æ–°è°ƒç”¨API...")
                        
                        # ç¡®ä¿ä½¿ç”¨æ–°tokené‡æ–°æ„å»ºheaders
                        new_headers = None
                        if use_intranet_token:
                            new_headers = {
                                "Content-Type": "application/json",
                                "Authorization": new_token
                            }
                        
                        # é‡æ–°è°ƒç”¨APIï¼ˆé€’å½’ï¼Œä½†ç¦ç”¨è‡ªåŠ¨é‡è¯•é¿å…æ— é™å¾ªç¯ï¼‰
                        return await call_api_with_timing(
                            url=url,
                            method=method,
                            json_data=json_data,
                            headers=new_headers,
                            timeout=timeout,
                            auto_retry_on_token_expire=False,  # ç¦ç”¨é‡è¯•é¿å…å¾ªç¯
                            use_intranet_token=False  # å·²ç»æ‰‹åŠ¨è®¾ç½®headersäº†ï¼Œä¸éœ€è¦å†æ¬¡è®¾ç½®
                        )
                    else:
                        logger.error(f"Tokenåˆ·æ–°å¤±è´¥: {new_token}")
                        api_logger.error(f"APIè°ƒç”¨å¤±è´¥(tokenåˆ·æ–°å¤±è´¥) - URL: {url}")
                        return {"error": f"Tokenè¿‡æœŸä¸”åˆ·æ–°å¤±è´¥: {new_token}", "code": 40003}, execution_time
                
                api_logger.info(f"APIè°ƒç”¨æˆåŠŸ - URL: {url} - è€—æ—¶: {execution_time:.4f}s")
                return result, execution_time
            elif response.status_code == 401 and should_auto_retry:
                # å¤„ç†HTTP 401çŠ¶æ€ç ï¼ˆè®¤è¯å¤±è´¥ï¼‰
                logger.warning("æ£€æµ‹åˆ°401çŠ¶æ€ç ï¼Œå°è¯•è‡ªåŠ¨åˆ·æ–°token...")
                
                # åˆ·æ–°token
                success, new_token = await refresh_intranet_token()
                
                if success:
                    logger.info("Tokenåˆ·æ–°æˆåŠŸï¼Œé‡æ–°è°ƒç”¨API...")
                    
                    # ç¡®ä¿ä½¿ç”¨æ–°tokené‡æ–°æ„å»ºheaders
                    new_headers = None
                    if use_intranet_token:
                        new_headers = {
                            "Content-Type": "application/json",
                            "Authorization": new_token
                        }
                    
                    # é‡æ–°è°ƒç”¨APIï¼ˆé€’å½’ï¼Œä½†ç¦ç”¨è‡ªåŠ¨é‡è¯•é¿å…æ— é™å¾ªç¯ï¼‰
                    return await call_api_with_timing(
                        url=url,
                        method=method,
                        json_data=json_data,
                        headers=new_headers,
                        timeout=timeout,
                        auto_retry_on_token_expire=False,  # ç¦ç”¨é‡è¯•é¿å…å¾ªç¯
                        use_intranet_token=False  # å·²ç»æ‰‹åŠ¨è®¾ç½®headersäº†ï¼Œä¸éœ€è¦å†æ¬¡è®¾ç½®
                    )
                else:
                    logger.error(f"Tokenåˆ·æ–°å¤±è´¥: {new_token}")
                    api_logger.error(f"APIè°ƒç”¨å¤±è´¥(tokenåˆ·æ–°å¤±è´¥) - URL: {url}")
                    return {"error": f"401è®¤è¯å¤±è´¥ä¸”tokenåˆ·æ–°å¤±è´¥: {new_token}", "status_code": 401}, execution_time
            else:
                error_detail = f"APIè°ƒç”¨å¤±è´¥ - URL: {url} - çŠ¶æ€ç : {response.status_code} - è€—æ—¶: {execution_time:.4f}s"
                if response.status_code == 401:
                    current_token_preview = INTRANET_AUTH_TOKEN[:30] + "..." if INTRANET_AUTH_TOKEN else "None"
                    error_detail += f" - å½“å‰tokené¢„è§ˆ: {current_token_preview}"
                api_logger.error(error_detail)
                return {"error": response.text, "status_code": response.status_code}, execution_time
                
    except Exception as e:
        execution_time = time.perf_counter() - start_time
        api_logger.error(f"APIè°ƒç”¨å¼‚å¸¸ - URL: {url} - é”™è¯¯: {str(e)} - è€—æ—¶: {execution_time:.4f}s")
        return {"error": str(e)}, execution_time

# ============ å·¥å…·å®šä¹‰ ============

@mcp.tool()
async def check_yaogan_environment(ctx: Context = None) -> str:
    """
    æ£€æŸ¥é¥æ„Ÿå¤§æ¥¼ç¯å¢ƒè¿é€šæ€§å’ŒæœåŠ¡çŠ¶æ€
    
    æ£€æŸ¥å‰ç«¯ã€åç«¯ã€è®¡ç®—é›†ç¾¤ã€MinIOç­‰æœåŠ¡çš„å¯è®¿é—®æ€§
    """
    operation = "æ£€æŸ¥é¥æ„Ÿå¤§æ¥¼ç¯å¢ƒ"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation}")
        
        check_results = {}
        
        # æ£€æŸ¥æœåŠ¡åˆ—è¡¨
        services_to_check = [
            {"name": "OGEå‰ç«¯", "url": f"{OGE_FRONTEND_URL}/health", "type": "frontend"},
            {"name": "è®¡ç®—é›†ç¾¤ä¸»èŠ‚ç‚¹", "url": f"{COMPUTE_CLUSTER_MASTER}/health", "type": "compute"},
            {"name": "Spark Master Web UI", "url": f"{COMPUTE_CLUSTER_MASTER}:9091", "type": "spark"},
            {"name": "Hadoop Web UI", "url": f"{COMPUTE_CLUSTER_MASTER}:8088", "type": "hadoop"},
            {"name": "HBase Web UI", "url": f"{COMPUTE_CLUSTER_MASTER}:16010", "type": "hbase"},
            {"name": "Livy API", "url": f"{LIVY_API_BASE_URL}/sessions", "type": "livy"},
            {"name": "MinIO Web UI", "url": f"{MINIO_ENDPOINT}/login", "type": "minio"},
        ]
        
        for service in services_to_check:
            try:
                async with httpx.AsyncClient(timeout=10) as client:
                    response = await client.get(service["url"])
                    check_results[service["name"]] = {
                        "status": "accessible" if response.status_code < 500 else "error",
                        "status_code": response.status_code,
                        "url": service["url"],
                        "type": service["type"]
                    }
            except Exception as e:
                check_results[service["name"]] = {
                    "status": "unreachable",
                    "error": str(e),
                    "url": service["url"],
                    "type": service["type"]
                }
        
        # è®¡ç®—å¥åº·æœåŠ¡æ•°é‡
        accessible_count = sum(1 for result in check_results.values() if result["status"] == "accessible")
        total_count = len(check_results)
        
        environment_status = {
            "environment": "é¥æ„Ÿå¤§æ¥¼",
            "network_segment": f"{YAOGAN_NETWORK_SEGMENT}.x",
            "accessible_services": accessible_count,
            "total_services": total_count,
            "health_ratio": f"{accessible_count}/{total_count}",
            "service_details": check_results,
            "environment_config": {
                "frontend": OGE_FRONTEND_URL,
                "compute_master": COMPUTE_CLUSTER_MASTER,
                "livy_api": LIVY_API_BASE_URL,
                "minio": MINIO_ENDPOINT
            }
        }
        
        result = Result.succ(
            data=environment_status,
            msg=f"{operation}å®Œæˆ - {accessible_count}/{total_count} ä¸ªæœåŠ¡å¯è®¿é—®",
            operation=operation,
            api_endpoint="environment"
        )
        
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆ")
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ - å¥åº·æœåŠ¡: {accessible_count}/{total_count}")
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            operation=operation
        )
        return result.model_dump_json()

@mcp.tool()
async def refresh_token(ctx: Context = None) -> str:
    """
    æ‰‹åŠ¨åˆ·æ–°å†…ç½‘è®¤è¯Token
    
    å½“é‡åˆ°tokenè¿‡æœŸé”™è¯¯(40003)æ—¶ï¼Œå¯ä»¥ä½¿ç”¨æ­¤å·¥å…·æ‰‹åŠ¨åˆ·æ–°token
    """
    operation = "åˆ·æ–°Token"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"æ‰‹åŠ¨æ‰§è¡Œ{operation}")
        
        success, token_or_error = await refresh_intranet_token()
        
        if success:
            result = Result.succ(
                data={
                    "new_token": token_or_error[:50] + "...",  # åªæ˜¾ç¤ºå‰50ä¸ªå­—ç¬¦
                    "token_length": len(token_or_error),
                    "updated_at": time.strftime("%Y-%m-%d %H:%M:%S")
                },
                msg=f"{operation}æˆåŠŸ",
                operation=operation,
                api_endpoint="auth"
            )
            
            if ctx:
                await ctx.session.send_log_message("info", f"{operation}æˆåŠŸï¼Œæ–°tokenå·²æ›´æ–°")
        else:
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {token_or_error}",
                operation=operation
            )
            
            if ctx:
                await ctx.session.send_log_message("error", f"{operation}å¤±è´¥: {token_or_error}")
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ - æˆåŠŸ: {success}")
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            operation=operation
        )
        return result.model_dump_json()

@mcp.tool()
async def check_token_status(ctx: Context = None) -> str:
    """
    æ£€æŸ¥å½“å‰å†…ç½‘è®¤è¯TokençŠ¶æ€
    
    ç”¨äºè°ƒè¯•tokené—®é¢˜ï¼Œæ˜¾ç¤ºå½“å‰tokençš„ä¿¡æ¯
    """
    global INTRANET_AUTH_TOKEN
    operation = "æ£€æŸ¥TokençŠ¶æ€"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation}")
        
        if INTRANET_AUTH_TOKEN:
            # å°è¯•è§£æJWT tokençš„æœ‰æ•ˆæœŸï¼ˆå¦‚æœæ˜¯JWTæ ¼å¼ï¼‰
            token_info = {
                "has_token": True,
                "token_preview": INTRANET_AUTH_TOKEN[:50] + "...",
                "token_length": len(INTRANET_AUTH_TOKEN),
                "starts_with_bearer": INTRANET_AUTH_TOKEN.startswith("Bearer "),
                "has_double_space": "Bearer  " in INTRANET_AUTH_TOKEN,  # æ£€æµ‹åŒç©ºæ ¼é—®é¢˜
                "current_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "bearer_prefix_length": len(INTRANET_AUTH_TOKEN.split(' ')[0]) if ' ' in INTRANET_AUTH_TOKEN else 0
            }
            
            # å¦‚æœæ˜¯JWT tokenï¼Œå°è¯•è§£æè¿‡æœŸæ—¶é—´
            if "Bearer " in INTRANET_AUTH_TOKEN:
                jwt_part = INTRANET_AUTH_TOKEN.replace("Bearer ", "")
                try:
                    import base64
                    import json
                    # ç®€å•è§£æJWT payloadï¼ˆä¸éªŒè¯ç­¾åï¼‰
                    parts = jwt_part.split('.')
                    if len(parts) >= 2:
                        # æ·»åŠ paddingå¦‚æœéœ€è¦
                        payload = parts[1]
                        payload += '=' * (4 - len(payload) % 4)
                        decoded = base64.b64decode(payload)
                        payload_data = json.loads(decoded)
                        
                        if 'exp' in payload_data:
                            exp_time = payload_data['exp']
                            exp_readable = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(exp_time))
                            token_info["expires_at"] = exp_readable
                            token_info["expires_timestamp"] = exp_time
                            token_info["is_expired"] = time.time() > exp_time
                        
                        if 'user_name' in payload_data:
                            token_info["username"] = payload_data['user_name']
                            
                except Exception as e:
                    token_info["parse_error"] = str(e)
            
            result = Result.succ(
                data=token_info,
                msg=f"{operation}æˆåŠŸ",
                operation=operation,
                api_endpoint="debug"
            )
        else:
            result = Result.failed(
                msg=f"{operation}: å½“å‰æ²¡æœ‰token",
                operation=operation
            )
        
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆ")
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ")
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            operation=operation
        )
        return result.model_dump_json()

@mcp.tool()
async def coverage_aspect_analysis(
    bbox: List[float],
    coverage_type: str = "Coverage",
    pretreatment: bool = True,
    product_value: str = "Platform:Product:ASTER_GDEM_DEM30",
    radius: int = 1,
    ctx: Context = None
) -> str:
    """
    å¡å‘åˆ†æ - åŸºäºDEMæ•°æ®è®¡ç®—å¡å‘ä¿¡æ¯
    
    Parameters:
    - bbox: è¾¹ç•Œæ¡†åæ ‡ [minLon, minLat, maxLon, maxLat]
    - coverage_type: è¦†ç›–ç±»å‹
    - pretreatment: æ˜¯å¦è¿›è¡Œé¢„å¤„ç†
    - product_value: äº§å“æ•°æ®æº
    - radius: è®¡ç®—åŠå¾„
    """
    operation = "å¡å‘åˆ†æ"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation} - è¾¹ç•Œæ¡†: {bbox}")
        
        # æ„å»ºç®—æ³•å‚æ•°
        algorithm_args = {
            "coverage": {
                "type": coverage_type,
                "pretreatment": pretreatment,
                "preParams": {"bbox": bbox},
                "value": [product_value]
            },
            "radius": radius
        }
        
        # è°ƒç”¨å†…ç½‘API
        api_payload = {
            "name": "Coverage.aspect",
            "args": algorithm_args,
            "dockerImageSource": "DOCKER_HUB"
        }
        
        api_result, execution_time = await call_api_with_timing(
            url=INTRANET_API_BASE_URL,
            json_data=api_payload,
            use_intranet_token=True
        )
        
        if "error" in api_result:
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {api_result.get('error')}",
                operation=operation
            )
        else:
            result = Result.succ(
                data=api_result,
                msg=f"{operation}æ‰§è¡ŒæˆåŠŸ",
                operation=operation,
                execution_time=execution_time,
                api_endpoint="intranet"
            )
        
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶{execution_time:.2f}ç§’")
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ - è€—æ—¶: {execution_time:.2f}ç§’")
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            operation=operation
        )
        return result.model_dump_json()

# spatial_intersection å·¥å…·å·²åˆ é™¤

# coverage_slope_analysis å·¥å…·å·²åˆ é™¤

# terrain_analysis_suite å·¥å…·å·²åˆ é™¤

# get_oauth_token å’Œ refresh_intranet_token å·¥å…·å·²åˆ é™¤

@mcp.tool()
async def process_single_tool_testing(
    # region_id: str = "ASTGTM_N28E056",
    # product_id: str = "ASTER_GDEM_DEM30", 
    # center_lon: float = 56.25,
    # center_lat: float = 28.40,
    # zoom_level: int = 11,
    wait_for_completion: bool = False,  # é»˜è®¤ç«‹å³è¿”å›ï¼Œé¿å…è¶…æ—¶
    ctx: Context = None
) -> str:
    """
    è¯¥å·¥å…·ä¸ºï¼šæ•´ä½“æµç¨‹å•ä¸ªå·¥å…·æµ‹è¯•
    è¯¥å·¥å…·ä¸ºæµ‹è¯•ä½¿ç”¨ï¼Œé™¤éç”¨æˆ·æŒ‡å®šæµ‹è¯•çš„è¦æ±‚ï¼Œå¦åˆ™ä¸åº”è¯¥è¢«è°ƒç”¨ã€‚
    è¯¥å·¥å…·æµ‹è¯•çš„æ˜¯æŒ‰ç…§ä»£ç æäº¤çš„æ–¹å¼ï¼Œæ‰§è¡Œæˆä¸ªæµç¨‹ï¼Œè¿”å›æœ€ç»ˆçš„ç»“æœã€‚
    è¯¥æ–¹å¼çš„å·¥ä½œæµç¨‹ï¼š
    1. æäº¤ä»»åŠ¡ï¼ˆç«‹å³è¿”å›DAG IDï¼‰
    2. ä½¿ç”¨è¿”å›çš„DAG IDè°ƒç”¨ query_task_status æŸ¥è¯¢è¿›åº¦
    3. æ‰§è¡ŒçŠ¶æ€ä¸ºstartingä¸runningï¼Œè¦è°ƒç”¨query_task_status è½®è®­è¿›åº¦ï¼Œæ¯5ç§’ä¸€æ¬¡ï¼Œç›´åˆ°çŠ¶æ€æˆåŠŸæˆ–è€…è½®è®­æ¬¡æ•°è¶…è¿‡10æ¬¡
    
    Parameters:
    - wait_for_completion: æ˜¯å¦ç­‰å¾…ä»»åŠ¡å®Œæˆ (é»˜è®¤: Falseï¼Œç«‹å³è¿”å›é¿å…è¶…æ—¶)
    
    è¿”å›ä¿¡æ¯åŒ…å«ï¼š
    - ä»»åŠ¡çŠ¶æ€å’ŒDAG ID
    - ä¸‹ä¸€æ­¥æ“ä½œæŒ‡å¼•
    - æŸ¥è¯¢çŠ¶æ€çš„å…·ä½“å‚æ•°
    """
    operation = "å±±ä¸œè€•åœ°æµå‡ºåˆ†æ"
    region_id: str = "ASTGTM_N28E056",
    product_id: str = "ASTER_GDEM_DEM30", 
    center_lon: float = 56.25,
    center_lat: float = 28.40,
    zoom_level: int = 11,
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation} - åŒºåŸŸ: {region_id}, äº§å“: {product_id}")
        
        # æ„å»ºOGEä»£ç 
        oge_code = f"""import oge

oge.initialize()
service = oge.Service()

dem = service.getCoverage(coverageID="{region_id}", productID="{product_id}")
aspect = service.getProcess("Coverage.aspect").execute(dem, 1)

vis_params = {{"min": -1, "max": 1, "palette": ["#808080", "#949494", "#a9a9a9", "#bdbebd", "#d3d3d3","#e9e9e9"]}}
aspect.styles(vis_params).export("aspect")
oge.mapclient.centerMap({center_lon}, {center_lat}, {zoom_level})"""
        
        logger.info(f"ç”Ÿæˆçš„OGEä»£ç é•¿åº¦: {len(oge_code)} å­—ç¬¦")
        
        # è°ƒç”¨execute_dag_workflowæ‰§è¡Œå®Œæ•´å·¥ä½œæµ
        workflow_result = await execute_dag_workflow(
            code=oge_code,
            task_name="shandong_farmland_outflow_analysis",
            filename="shandong_aspect_analysis",
            auto_submit=True,
            wait_for_completion=wait_for_completion,
            check_interval=10,          # æ¯10ç§’è½®è¯¢ä¸€æ¬¡
            max_wait_time=1800,         # 30åˆ†é’Ÿè¶…æ—¶
            ctx=ctx
        )
        
        # è§£æworkflowç»“æœ
        import json
        workflow_data = json.loads(workflow_result)
        
        if workflow_data.get("success"):
            # æå–å…³é”®ä¿¡æ¯
            workflow_details = workflow_data.get("data", {})
            final_status = workflow_details.get("final_status", "unknown")
            
            result_data = {
                "region_id": region_id,
                "product_id": product_id,
                "analysis_type": "aspect_analysis",
                "map_center": {"lon": center_lon, "lat": center_lat, "zoom": zoom_level},
                "workflow_status": final_status,
                "execution_steps": workflow_details.get("steps", []),
                "execution_times": workflow_details.get("execution_times", {}),
                "dag_info": {
                    "dag_ids": workflow_details.get("dag_ids", []),
                    "primary_dag_id": workflow_details.get("dag_ids", ["unknown"])[0],
                    "task_name": "shandong_farmland_outflow_analysis"
                },
                "next_action": {
                    "tool_name": "query_task_status",
                    "parameters": {
                        "dag_id": workflow_details.get("dag_ids", ["unknown"])[0]
                    },
                    "description": "æŸ¥è¯¢ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€"
                } if final_status == "submitted" else None
            }
            
            if final_status == "completed":
                msg = f"{operation}æ‰§è¡ŒæˆåŠŸ - DEMå¡å‘åˆ†æå·²å®Œæˆå¹¶å¯è§†åŒ–"
            elif final_status == "submitted":
                primary_dag_id = workflow_details.get("dag_ids", ["unknown"])[0]
                msg = f"{operation}ä»»åŠ¡å·²æäº¤ - DAG ID: {primary_dag_id}\n" + \
                      f"ğŸ’¡ è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥è¯¢è¿›åº¦ï¼š\n" + \
                      f"query_task_status(dag_id=\"{primary_dag_id}\")"
            else:
                msg = f"{operation}æ‰§è¡Œå®Œæˆ - çŠ¶æ€: {final_status}"
            
            result = Result.succ(
                data=result_data,
                msg=msg,
                operation=operation,
                api_endpoint="dag_workflow"
            )
        else:
            # å·¥ä½œæµæ‰§è¡Œå¤±è´¥
            error_msg = workflow_data.get("msg", "å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {error_msg}",
                operation=operation
            )
            result.data = workflow_data.get("data")
        
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆ")
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ - æœ€ç»ˆçŠ¶æ€: {final_status}")
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            operation=operation
        )
        return result.model_dump_json()

@mcp.tool()
async def farmland_outflow(
    slope_threshold: int = 4,           # å¡åº¦ç­‰çº§é˜ˆå€¼ï¼Œ4å¯¹åº”15åº¦
    fragment_area_threshold: float = 3333.3333,  # ç»†ç¢åŒ–é¢ç§¯é˜ˆå€¼ï¼ˆ5äº©=3333.33å¹³æ–¹ç±³ï¼‰
    buffer_distance: float = 10.0,     # ç¼“å†²åŒºè·ç¦»ï¼ˆç±³ï¼‰
    peripheral_area_threshold: float = 6666.6667,  # å‘¨è¾¹é¢ç§¯é˜ˆå€¼ï¼ˆ10äº©ï¼‰
    wait_for_completion: bool = False,  # é»˜è®¤ç«‹å³è¿”å›ï¼Œé¿å…è¶…æ—¶
    ctx: Context = None
) -> str:
    """
    è€•åœ°æµå‡ºåˆ†æ - åŸºäºå¤šé‡çº¦æŸæ¡ä»¶çš„è€•åœ°ä¿æŠ¤åˆ†æ
    
    å·¥ä½œæµç¨‹ï¼š
    1. æäº¤ä»»åŠ¡ï¼ˆç«‹å³è¿”å›DAG IDï¼‰
    2. ä½¿ç”¨è¿”å›çš„DAG IDè°ƒç”¨ query_task_status æŸ¥è¯¢è¿›åº¦
    3. æ‰§è¡ŒçŠ¶æ€ä¸ºstartingä¸runningæ˜¯ï¼Œè¦è°ƒç”¨query_task_status è½®è®­è¿›åº¦ï¼Œæ¯5ç§’ä¸€æ¬¡ï¼Œç›´åˆ°çŠ¶æ€æˆåŠŸæˆ–è€…è½®è®­æ¬¡æ•°è¶…è¿‡10æ¬¡
    
    Parameters:
    - slope_threshold: å¡åº¦ç­‰çº§é˜ˆå€¼ (é»˜è®¤: 4, å¯¹åº”15åº¦)
    - fragment_area_threshold: ç»†ç¢åŒ–é¢ç§¯é˜ˆå€¼ï¼Œå¹³æ–¹ç±³ (é»˜è®¤: 3333.33, çº¦5äº©)
    - buffer_distance: ç¼“å†²åŒºè·ç¦»ï¼Œç±³ (é»˜è®¤: 10.0)
    - peripheral_area_threshold: å‘¨è¾¹é¢ç§¯é˜ˆå€¼ï¼Œå¹³æ–¹ç±³ (é»˜è®¤: 6666.67, çº¦10äº©)
    - wait_for_completion: æ˜¯å¦ç­‰å¾…ä»»åŠ¡å®Œæˆ (é»˜è®¤: Falseï¼Œç«‹å³è¿”å›é¿å…è¶…æ—¶)
    
    è¿”å›ä¿¡æ¯åŒ…
    - ä»»åŠ¡çŠ¶æ€å’ŒDAG ID
    - ä¸‹ä¸€æ­¥æ“ä½œæŒ‡å¼•
    - æŸ¥è¯¢çŠ¶æ€çš„å…·ä½“å‚æ•°
    """
    operation = "è€•åœ°æµå‡ºåˆ†æ"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation} - å¡åº¦é˜ˆå€¼: {slope_threshold}, é¢ç§¯é˜ˆå€¼: {fragment_area_threshold}")
        
        # æ„å»ºOGEä»£ç 
        oge_code = f"""import oge
oge.initialize()

service = oge.Service.initialize()
query = r"SELECT * FROM shp_guotubiangeng WHERE DLMC IN ('æ—±åœ°', 'æ°´æµ‡åœ°', 'æ°´ç”°')"
cultivated = service.getProcess("FeatureCollection.runBigQuery").execute(query, "geom") #è€•åœ° 

# é‡ç‚¹ç®¡æ§åŒºåŸŸ å¤§äº15åº¦è€•åœ° å‡ ä½•
slope = service.getFeatureCollection("shp_podu") #å¡åº¦ GCS_China_Geodetic_Coordinate_System_2000
slope_morethan15_ = service.getProcess("FeatureCollection.filterMetadata").execute(slope, "pdjb", "greater_than", {slope_threshold}) #è¶…è¿‡15åº¦çš„è€•åœ°
slope_morethan15 = service.getProcess("FeatureCollection.reproject").execute(slope_morethan15_, "EPSG:4527")
urban_ = service.getFeatureCollection("shp_chengzhenkaifa") #åŸé•‡å¼€å‘è¾¹ç•Œ
urban = service.getProcess("FeatureCollection.reproject").execute(urban_, "EPSG:4527")
nature_ = service.getFeatureCollection("shp_ziranbaohu") #è‡ªç„¶ä¿æŠ¤åœ°
nature = service.getProcess("FeatureCollection.reproject").execute(nature_, "EPSG:4527")
ecology_ = service.getFeatureCollection("shp_shengtaibaohu") #ç”Ÿæ€ä¿æŠ¤çº¢çº¿
ecology = service.getProcess("FeatureCollection.reproject").execute(ecology_, "EPSG:4527")

# cultivated_protected = service.getProcess("FeatureCollection.reproject").execute(cultivated, "EPSG:4527")

urban_intersection = service.getProcess("FeatureCollection.intersection").execute(cultivated, urban) #æµå‡º1
urban_erase = service.getProcess("FeatureCollection.erase").execute(cultivated, urban)
nature_intersection = service.getProcess("FeatureCollection.intersection").execute(urban_erase, nature) #æµå‡º2
nature_erase = service.getProcess("FeatureCollection.erase").execute(urban_erase, nature)
ecology_intersection = service.getProcess("FeatureCollection.intersection").execute(nature_erase, ecology) ##æµå‡º3
ecology_erase = service.getProcess("FeatureCollection.erase").execute(nature_erase, ecology)
slope_intersection = service.getProcess("FeatureCollection.intersection").execute(ecology_erase, slope_morethan15) #æµå‡º4
slope_erase = service.getProcess("FeatureCollection.erase").execute(ecology_erase, slope_morethan15)

#ç­›é€‰ç»†ç¢åŒ–è€•åœ°
cultivated1_area = service.getProcess("FeatureCollection.area").execute(slope_erase) #å¢åŠ areaå­—æ®µ
cultivated1_lessthan5 = service.getProcess("FeatureCollection.filterMetadata").execute(cultivated1_area, "area", "less_than", {fragment_area_threshold})
cultivated1_buffer = service.getProcess("FeatureCollection.buffer").execute(cultivated1_lessthan5, {buffer_distance})
cultivated1_join = service.getProcess("FeatureCollection.spatialJoinOneToOne").execute(cultivated1_buffer, cultivated1_area, "buffer", "geom", True, "Intersects", ["area"], ["sum"])
cultivated1_subtract = service.getProcess("FeatureCollection.subtract").execute(cultivated1_join, "area_sum", "area", "area_peri")
deprecated1 = service.getProcess("FeatureCollection.filterMetadata").execute(cultivated1_subtract, "area_peri", "less_than", "{peripheral_area_threshold}") #æµå‡º5

urban_intersection_reason = service.getProcess("FeatureCollection.constantColumn").execute(urban_intersection, "reason", "urban")
nature_intersection_reason = service.getProcess("FeatureCollection.constantColumn").execute(nature_intersection, "reason", "nature")
ecology_intersection_reason = service.getProcess("FeatureCollection.constantColumn").execute(ecology_intersection, "reason", "ecology")
slope_intersection_reason = service.getProcess("FeatureCollection.constantColumn").execute(slope_intersection, "reason", "slope")
deprecated1_reason = service.getProcess("FeatureCollection.constantColumn").execute(deprecated1, "reason", "fragmented")

deprecated = service.getProcess("FeatureCollection.mergeAll").execute([urban_intersection_reason,nature_intersection_reason,ecology_intersection_reason,slope_intersection_reason,deprecated1_reason]) #éœ€è¦æµå‡ºçš„è€•åœ°
deprecated_CGCS2000 = service.getProcess("FeatureCollection.reproject").execute(deprecated, "EPSG:4490")
deprecated_CGCS2000.export("cultivated_protected")"""
        
        logger.info(f"ç”Ÿæˆçš„OGEä»£ç é•¿åº¦: {len(oge_code)} å­—ç¬¦")
        
        # è°ƒç”¨execute_dag_workflowæ‰§è¡Œå®Œæ•´å·¥ä½œæµ
        workflow_result = await execute_dag_workflow(
            code=oge_code,
            task_name="farmland_outflow_analysis",
            filename="farmland_outflow_result",
            auto_submit=True,
            wait_for_completion=wait_for_completion,
            check_interval=10,          # æ¯10ç§’è½®è¯¢ä¸€æ¬¡
            max_wait_time=1800,         # 30åˆ†é’Ÿè¶…æ—¶
            ctx=ctx
        )
        
        # è§£æworkflowç»“æœ
        import json
        workflow_data = json.loads(workflow_result)
        
        if workflow_data.get("success"):
            # æå–å…³é”®ä¿¡æ¯
            workflow_details = workflow_data.get("data", {})
            final_status = workflow_details.get("final_status", "unknown")
            
            result_data = {
                "analysis_type": "farmland_outflow_analysis",
                "parameters": {
                    "slope_threshold": slope_threshold,
                    "fragment_area_threshold": fragment_area_threshold,
                    "buffer_distance": buffer_distance,
                    "peripheral_area_threshold": peripheral_area_threshold
                },
                "workflow_status": final_status,
                "execution_steps": workflow_details.get("steps", []),
                "execution_times": workflow_details.get("execution_times", {}),
                "dag_info": {
                    "dag_ids": workflow_details.get("dag_ids", []),
                    "primary_dag_id": workflow_details.get("dag_ids", ["unknown"])[0],
                    "task_name": "farmland_outflow_analysis"
                },
                "outflow_categories": [
                    {"type": "urban", "description": "åŸé•‡å¼€å‘è¾¹ç•Œå†…è€•åœ°"},
                    {"type": "nature", "description": "è‡ªç„¶ä¿æŠ¤åœ°å†…è€•åœ°"},
                    {"type": "ecology", "description": "ç”Ÿæ€ä¿æŠ¤çº¢çº¿å†…è€•åœ°"},
                    {"type": "slope", "description": f"å¡åº¦å¤§äº{slope_threshold}çº§çš„è€•åœ°"},
                    {"type": "fragmented", "description": f"ç»†ç¢åŒ–è€•åœ°ï¼ˆ<{fragment_area_threshold/666.67:.1f}äº©ï¼‰"}
                ],
                "next_action": {
                    "tool_name": "query_task_status",
                    "parameters": {
                        "dag_id": workflow_details.get("dag_ids", ["unknown"])[0]
                    },
                    "description": "æŸ¥è¯¢ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€"
                } if final_status == "submitted" else None
            }
            
            if final_status == "completed":
                msg = f"{operation}æ‰§è¡ŒæˆåŠŸ - è€•åœ°æµå‡ºåˆ†æå·²å®Œæˆï¼Œè¯†åˆ«å‡º5ç±»éœ€è¦é€€å‡ºçš„è€•åœ°"
            elif final_status == "submitted":
                primary_dag_id = workflow_details.get("dag_ids", ["unknown"])[0]
                msg = f"{operation}ä»»åŠ¡å·²æäº¤ - DAG ID: {primary_dag_id}\n" + \
                      f"ğŸ’¡ è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥è¯¢è¿›åº¦ï¼š\n" + \
                      f"query_task_status(dag_id=\"{primary_dag_id}\")\n" + \
                      f"ğŸ“Š åˆ†æå‚æ•°ï¼šå¡åº¦é˜ˆå€¼{slope_threshold}çº§ï¼Œé¢ç§¯é˜ˆå€¼{fragment_area_threshold/666.67:.1f}äº©"
            else:
                msg = f"{operation}æ‰§è¡Œå®Œæˆ - çŠ¶æ€: {final_status}"
            
            result = Result.succ(
                data=result_data,
                msg=msg,
                operation=operation,
                api_endpoint="dag_workflow"
            )
        else:
            # å·¥ä½œæµæ‰§è¡Œå¤±è´¥
            error_msg = workflow_data.get("msg", "å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {error_msg}",
                operation=operation
            )
            result.data = workflow_data.get("data")
        
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆ")
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ - æœ€ç»ˆçŠ¶æ€: {final_status}")
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            operation=operation
        )
        return result.model_dump_json()

@mcp.tool()
async def run_big_query(
    # query: str,
    # geometry_column: str = "geom",
    ctx: Context = None
) -> str:
    """
    æŸ¥è¯¢å±±ä¸œçœè€•åœ°çŸ¢é‡,åªä¼šè¿”å›æ•°æ®çš„æ ‡è¯†ï¼Œé€šè¿‡æ ‡è¯†åç»­å¯ä»¥è®¿é—®ç»“æœæ•°æ®
    
    Parameters:
        æ— å‚æ•°
    """
    operation = "å¤§æ•°æ®æŸ¥è¯¢"
    query = "SELECT * FROM shp_guotubiangeng WHERE DLMC IN ('æ—±åœ°', 'æ°´æµ‡åœ°', 'æ°´ç”°')"
    geometry_column = "geom"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation} - æŸ¥è¯¢: {query[:100]}...")
        
        # æ„å»ºç®—æ³•å‚æ•°
        algorithm_args = {
            "query": query,
            "geometryColumn": geometry_column
        }
        
        # è°ƒç”¨å†…ç½‘API
        api_payload = {
            "name": "FeatureCollection.runBigQuery",
            "args": algorithm_args,
            "dockerImageSource": "DOCKER_HUB"
        }
        
        api_result, execution_time = await call_api_with_timing(
            url=INTRANET_API_BASE_URL,
            json_data=api_payload,
            use_intranet_token=True
        )
        
        if "error" in api_result:
            error_detail = api_result.get('error', 'æœªçŸ¥é”™è¯¯')
            status_code = api_result.get('status_code', 'æœªçŸ¥çŠ¶æ€ç ')
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {error_detail} (çŠ¶æ€ç : {status_code})",
                operation=operation
            )
        else:
            result = Result.succ(
                data=api_result,
                msg=f"{operation}æ‰§è¡ŒæˆåŠŸ",
                operation=operation,
                execution_time=execution_time,
                api_endpoint="intranet"
            )
        
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶{execution_time:.2f}ç§’")
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ - è€—æ—¶: {execution_time:.2f}ç§’")
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            operation=operation
        )
        return result.model_dump_json()


# ============ DAGæ‰¹å¤„ç†å·¥å…· ============

@mcp.tool()
async def execute_code_to_dag(
    code: str,
    user_id: str = DEFAULT_USER_ID,
    sample_name: str = "",
    auth_token: str = None,
    ctx: Context = None
) -> str:
    """
    å°†ä»£ç è½¬åŒ–ä¸ºDAGç”Ÿæˆä»»åŠ¡
    
    Parameters:
    - code: è¦æ‰§è¡Œçš„OGEä»£ç 
    - user_id: ç”¨æˆ·UUID
    - sample_name: ç¤ºä¾‹ä»£ç åç§°ï¼ˆå¯ä¸ºç©ºï¼‰
    - auth_token: è®¤è¯Tokenï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€Tokenï¼‰
    """
    operation = "ä»£ç è½¬DAGä»»åŠ¡"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation}")
        
        # æ„å»ºAPI URL
        api_url = f"{DAG_API_BASE_URL}/executeCode"
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        request_data = {
            "code": code,
            "userId": user_id,
            "sampleName": sample_name
        }
        
        # å‡†å¤‡è®¤è¯
        use_custom_token = bool(auth_token)
        final_headers = None
        
        if use_custom_token:
            if not auth_token.startswith("Bearer "):
                auth_token = f"Bearer {auth_token}"
            final_headers = {
                "Content-Type": "application/json",
                "Authorization": auth_token
            }
        
        logger.info(f"è°ƒç”¨API: {api_url}")
        logger.info(f"è¯·æ±‚æ•°æ®: userId={user_id}, sampleName={sample_name}")
        
        # è°ƒç”¨API
        api_result, execution_time = await call_api_with_timing(
            url=api_url,
            method="POST",
            json_data=request_data,
            headers=final_headers,
            timeout=300,     # 5åˆ†é’Ÿè¶…æ—¶ï¼ŒDAGåˆ›å»ºå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
            use_intranet_token=not use_custom_token
        )
        
        if "error" not in api_result:
            # æå–DAGä¿¡æ¯
            dags = api_result.get("dags", {})
            space_params = api_result.get("spaceParams", {})
            log_info = api_result.get("log", "")
            
            # æå–DAG ID
            dag_ids = []
            for key, value in dags.items():
                if isinstance(value, dict):
                    dag_ids.append(key)
                elif isinstance(value, str):
                    dag_ids.append(value)
            
            result_data = {
                "dags": dags,
                "dag_ids": dag_ids,
                "space_params": space_params,
                "log": log_info,
                "user_id": user_id,
                "sample_name": sample_name
            }
            
            result = Result.succ(
                data=result_data,
                msg=f"{operation}æˆåŠŸï¼Œç”Ÿæˆäº†{len(dag_ids)}ä¸ªDAGä»»åŠ¡",
                operation=operation,
                execution_time=execution_time,
                api_endpoint="dag"
            )
            
            logger.info(f"{operation}æˆåŠŸ - ç”ŸæˆDAGæ•°é‡: {len(dag_ids)}")
            
        else:
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {api_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                operation=operation
            )
        
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶{execution_time:.2f}ç§’")
        
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            operation=operation
        )
        return result.model_dump_json()

@mcp.tool()
async def submit_batch_task(
    dag_id: str,
    task_name: str = None,
    filename: str = None,
    crs: str = "EPSG:4326",
    scale: str = "1000",
    format: str = "tif",
    username: str = DEFAULT_USERNAME,
    script: str = "",
    auth_token: str = None,
    ctx: Context = None
) -> str:
    """
    æäº¤æ‰¹å¤„ç†ä»»åŠ¡è¿è¡Œ
    
    Parameters:
    - dag_id: DAGä»»åŠ¡ID
    - task_name: ä»»åŠ¡åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰
    - filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰
    - crs: åæ ‡å‚è€ƒç³»ç»Ÿ
    - scale: æ¯”ä¾‹å°º
    - format: è¾“å‡ºæ ¼å¼
    - username: ç”¨æˆ·å
    - script: è„šæœ¬ä»£ç 
    - auth_token: è®¤è¯Tokenï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€Tokenï¼‰
    """
    operation = "æäº¤æ‰¹å¤„ç†ä»»åŠ¡"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation} - DAG ID: {dag_id}")
        
        # æ„å»ºAPI URL
        api_url = f"{DAG_API_BASE_URL}/addTaskRecord"
        
        # ç”Ÿæˆé»˜è®¤ä»»åŠ¡åå’Œæ–‡ä»¶åï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if not task_name:
            timestamp = time.strftime("%Y_%m_%d_%H_%M_%S")
            task_name = f"task_{timestamp}"
            
        if not filename:
            timestamp = time.strftime("%Y_%m_%d_%H_%M_%S")
            filename = f"file_{timestamp}"
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        request_data = {
            "taskName": task_name,
            "crs": crs,
            "scale": scale,
            "filename": filename,
            "format": format,
            "id": dag_id,
            "userName": username,
            "script": script
        }
        
        # å‡†å¤‡è®¤è¯
        use_custom_token = bool(auth_token)
        final_headers = None
        
        if use_custom_token:
            if not auth_token.startswith("Bearer "):
                auth_token = f"Bearer {auth_token}"
            final_headers = {
                "Content-Type": "application/json",
                "Authorization": auth_token
            }
        
        logger.info(f"è°ƒç”¨API: {api_url}")
        logger.info(f"è¯·æ±‚æ•°æ®: taskName={task_name}, dagId={dag_id}")
        
        # è°ƒç”¨API
        api_result, execution_time = await call_api_with_timing(
            url=api_url,
            method="POST",
            json_data=request_data,
            headers=final_headers,
            timeout=300,     # 5åˆ†é’Ÿè¶…æ—¶ï¼Œä»»åŠ¡æäº¤å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
            use_intranet_token=not use_custom_token
        )
        
        if "error" not in api_result:
            # æ£€æŸ¥APIå“åº”æ ¼å¼
            if api_result.get("code") == 200:
                task_data = api_result.get("data", {})
                
                result_data = {
                    "batch_session_id": task_data.get("batchSessionId"),
                    "task_id": task_data.get("id"),
                    "dag_id": task_data.get("dagId"),
                    "task_name": task_data.get("taskName"),
                    "state": task_data.get("state"),
                    "filename": task_data.get("filename"),
                    "format": task_data.get("format"),
                    "scale": task_data.get("scale"),
                    "crs": task_data.get("crs"),
                    "user_id": task_data.get("userId"),
                    "username": task_data.get("userName"),
                    "folder": task_data.get("folder"),
                    "api_response": api_result
                }
                
                result = Result.succ(
                    data=result_data,
                    msg=f"{operation}æˆåŠŸï¼Œä»»åŠ¡çŠ¶æ€: {task_data.get('state', 'unknown')}",
                    operation=operation,
                    execution_time=execution_time,
                    api_endpoint="dag"
                )
                
                logger.info(f"{operation}æˆåŠŸ - ä»»åŠ¡ID: {task_data.get('id')}, çŠ¶æ€: {task_data.get('state')}")
                
            else:
                result = Result.failed(
                    msg=f"{operation}å¤±è´¥: {api_result.get('msg', 'æœªçŸ¥é”™è¯¯')}",
                    operation=operation
                )
        else:
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {api_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                operation=operation
            )
        
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶{execution_time:.2f}ç§’")
        
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            operation=operation
        )
        return result.model_dump_json()

@mcp.tool()
async def query_task_status(
    dag_id: str,
    auth_token: str = None,
    ctx: Context = None
) -> str:
    """
    æŸ¥è¯¢æ‰¹å¤„ç†ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
    
    Parameters:
    - dag_id: DAGä»»åŠ¡ID
    - auth_token: è®¤è¯Tokenï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€Tokenï¼‰
    """
    operation = "æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation} - DAG ID: {dag_id}")
        
        # æ„å»ºAPI URL
        api_url = f"{DAG_API_BASE_URL}/getState"
        
        # å‡†å¤‡è®¤è¯
        use_custom_token = bool(auth_token)
        final_headers = None
        
        if use_custom_token:
            if not auth_token.startswith("Bearer "):
                auth_token = f"Bearer {auth_token}"
            final_headers = {
                "Content-Type": "application/json",
                "Authorization": auth_token
            }
        
        # æ„å»ºæŸ¥è¯¢å‚æ•°
        params = {"dagId": dag_id}
        
        logger.info(f"è°ƒç”¨API: {api_url}?dagId={dag_id}")
        
        # è°ƒç”¨API - éœ€è¦ç‰¹æ®Šå¤„ç†GETè¯·æ±‚
        if use_custom_token:
            # ä½¿ç”¨è‡ªå®šä¹‰token
            start_time = time.perf_counter()
            
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.get(
                    api_url,
                    params=params,
                    headers=final_headers
                )
                
                execution_time = time.perf_counter() - start_time
                
                if response.status_code == 200:
                    # APIè¿”å›çš„å¯èƒ½æ˜¯ç®€å•çš„å­—ç¬¦ä¸²çŠ¶æ€
                    response_text = response.text.strip()
                    
                    if not response_text:
                        # ç©ºå“åº”ï¼Œå¯èƒ½è¡¨ç¤ºä»»åŠ¡ä¸å­˜åœ¨æˆ–æŸ¥è¯¢å‡ºé”™
                        status_data = "unknown"
                    else:
                        try:
                            status_data = response.json()
                        except:
                            # å¦‚æœä¸æ˜¯JSONï¼Œåˆ™æ˜¯çº¯æ–‡æœ¬çŠ¶æ€
                            status_data = response_text
                    
                    # ç¡®ä¿status_dataæ˜¯å­—ç¬¦ä¸²å½¢å¼
                    if isinstance(status_data, dict):
                        status_str = status_data.get("status", str(status_data))
                    else:
                        status_str = str(status_data)
                    
                    result_data = {
                        "dag_id": dag_id,
                        "status": status_str,
                        "is_completed": status_str in ["success", "completed"],
                        "is_running": status_str in ["running", "starting"],
                        "is_failed": status_str in ["failed", "error"],
                        "raw_response": status_data,
                        "response_length": len(response_text)
                    }
                    
                    result = Result.succ(
                        data=result_data,
                        msg=f"{operation}æˆåŠŸï¼Œå½“å‰çŠ¶æ€: {status_data}",
                        operation=operation,
                        execution_time=execution_time,
                        api_endpoint="dag"
                    )
                    
                    logger.info(f"{operation}æˆåŠŸ - DAG ID: {dag_id}, çŠ¶æ€: {status_data}")
                    
                else:
                    error_msg = f"HTTP {response.status_code}: {response.text}"
                    result = Result.failed(
                        msg=f"{operation}å¤±è´¥: {error_msg}",
                        operation=operation
                    )
                    logger.error(f"{operation}å¤±è´¥ - {error_msg}")
        else:
            # ä½¿ç”¨å†…ç½‘token - è½¬æ¢ä¸ºPOSTè¯·æ±‚ä»¥æ”¯æŒtokenåˆ·æ–°
            get_params = {"dagId": dag_id}
            
            api_result, execution_time = await call_api_with_timing(
                url=api_url,
                method="GET",
                headers={"params": get_params},  # ä¼ é€’GETå‚æ•°
                timeout=30,
                use_intranet_token=True
            )
            
            if "error" not in api_result:
                status_data = api_result
                if isinstance(status_data, dict):
                    status_data = status_data.get("status", "unknown")
                
                result_data = {
                    "dag_id": dag_id,
                    "status": status_data,
                    "is_completed": status_data in ["success", "completed"],
                    "is_running": status_data in ["running", "starting"],
                    "is_failed": status_data in ["failed", "error"],
                    "raw_response": status_data
                }
                
                result = Result.succ(
                    data=result_data,
                    msg=f"{operation}æˆåŠŸï¼Œå½“å‰çŠ¶æ€: {status_data}",
                    operation=operation,
                    execution_time=execution_time,
                    api_endpoint="dag"
                )
                
                logger.info(f"{operation}æˆåŠŸ - DAG ID: {dag_id}, çŠ¶æ€: {status_data}")
                
            else:
                result = Result.failed(
                    msg=f"{operation}å¤±è´¥: {api_result.get('error')}",
                    operation=operation
                )
                logger.error(f"{operation}å¤±è´¥ - {api_result.get('error')}")
        
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶{execution_time:.2f}ç§’")
        
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            operation=operation
        )
        return result.model_dump_json()

@mcp.tool()
async def execute_dag_workflow(
    code: str,
    user_id: str = DEFAULT_USER_ID,
    sample_name: str = "",
    task_name: str = None,
    filename: str = None,
    crs: str = "EPSG:4326",
    scale: str = "1000",
    format: str = "tif",
    username: str = DEFAULT_USERNAME,
    auth_token: str = None,
    auto_submit: bool = True,
    wait_for_completion: bool = False,
    check_interval: int = 15,     # é»˜è®¤15ç§’æ£€æŸ¥ä¸€æ¬¡
    max_wait_time: int = 1800,    # é»˜è®¤30åˆ†é’Ÿè¶…æ—¶
    ctx: Context = None
) -> str:
    """
    æ‰§è¡Œå®Œæ•´çš„DAGæ‰¹å¤„ç†å·¥ä½œæµï¼šä»£ç è½¬DAG -> æäº¤ä»»åŠ¡ -> (å¯é€‰)ç­‰å¾…å®Œæˆ
    
    Parameters:
    - code: OGEä»£ç 
    - user_id: ç”¨æˆ·UUID
    - sample_name: ç¤ºä¾‹ä»£ç åç§°
    - task_name: ä»»åŠ¡åç§°ï¼ˆå¯é€‰ï¼‰
    - filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
    - crs: åæ ‡å‚è€ƒç³»ç»Ÿ
    - scale: æ¯”ä¾‹å°º
    - format: è¾“å‡ºæ ¼å¼
    - username: ç”¨æˆ·å
    - auth_token: è®¤è¯Tokenï¼ˆå¯é€‰ï¼‰
    - auto_submit: æ˜¯å¦è‡ªåŠ¨æäº¤ä»»åŠ¡
    - wait_for_completion: æ˜¯å¦ç­‰å¾…ä»»åŠ¡å®Œæˆ
    - check_interval: çŠ¶æ€æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    - max_wait_time: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    """
    operation = "DAGæ‰¹å¤„ç†å·¥ä½œæµ"
    workflow_start_time = time.perf_counter()
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation}")
        
        workflow_results = {
            "steps": [],
            "final_status": "unknown",
            "dag_ids": [],
            "task_info": None,
            "execution_times": {}
        }
        
        # æ­¥éª¤1: ä»£ç è½¬DAG
        if ctx:
            await ctx.session.send_log_message("info", "æ­¥éª¤1: ä»£ç è½¬æ¢ä¸ºDAG...")
        
        dag_result_json = await execute_code_to_dag(
            code=code,
            user_id=user_id,
            sample_name=sample_name,
            auth_token=auth_token,
            ctx=ctx
        )
        
        dag_result = json.loads(dag_result_json)
        workflow_results["steps"].append({
            "step": 1,
            "name": "ä»£ç è½¬DAG",
            "success": dag_result.get("success", False),
            "result": dag_result
        })
        
        if not dag_result.get("success"):
            workflow_results["final_status"] = "failed_at_dag_creation"
            result = Result.failed(
                msg=f"{operation}å¤±è´¥ï¼šä»£ç è½¬DAGæ­¥éª¤å¤±è´¥",
                operation=operation
            )
            result.data = workflow_results
            return result.model_dump_json()
        
        # è·å–DAGä¿¡æ¯
        dag_data = dag_result.get("data", {})
        dag_ids = dag_data.get("dag_ids", [])
        workflow_results["dag_ids"] = dag_ids
        
        if not dag_ids:
            workflow_results["final_status"] = "no_dag_generated"
            result = Result.failed(
                msg=f"{operation}å¤±è´¥ï¼šæœªç”ŸæˆDAGä»»åŠ¡",
                operation=operation
            )
            result.data = workflow_results
            return result.model_dump_json()
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªDAG ID
        primary_dag_id = dag_ids[0]
        logger.info(f"ä½¿ç”¨DAG ID: {primary_dag_id}")
        
        if auto_submit:
            # æ­¥éª¤2: æäº¤æ‰¹å¤„ç†ä»»åŠ¡
            if ctx:
                await ctx.session.send_log_message("info", f"æ­¥éª¤2: æäº¤æ‰¹å¤„ç†ä»»åŠ¡ (DAG: {primary_dag_id})...")
            
            submit_result_json = await submit_batch_task(
                dag_id=primary_dag_id,
                task_name=task_name,
                filename=filename,
                crs=crs,
                scale=scale,
                format=format,
                username=username,
                script=code,
                auth_token=auth_token,
                ctx=ctx
            )
            
            submit_result = json.loads(submit_result_json)
            workflow_results["steps"].append({
                "step": 2,
                "name": "æäº¤æ‰¹å¤„ç†ä»»åŠ¡",
                "success": submit_result.get("success", False),
                "result": submit_result
            })
            
            if not submit_result.get("success"):
                workflow_results["final_status"] = "failed_at_task_submission"
                result = Result.failed(
                    msg=f"{operation}å¤±è´¥ï¼šä»»åŠ¡æäº¤æ­¥éª¤å¤±è´¥",
                    operation=operation
                )
                result.data = workflow_results
                return result.model_dump_json()
            
            # è·å–ä»»åŠ¡ä¿¡æ¯
            task_data = submit_result.get("data", {})
            workflow_results["task_info"] = task_data
            
            if wait_for_completion:
                # æ­¥éª¤3: ç­‰å¾…ä»»åŠ¡å®Œæˆ
                if ctx:
                    await ctx.session.send_log_message("info", f"æ­¥éª¤3: ç­‰å¾…ä»»åŠ¡å®Œæˆ...")
                
                waited_time = 0
                final_status = "unknown"
                
                while waited_time < max_wait_time:
                    status_result_json = await query_task_status(
                        dag_id=primary_dag_id,
                        auth_token=auth_token,
                        ctx=None  # é¿å…è¿‡å¤šæ—¥å¿—
                    )
                    
                    status_result = json.loads(status_result_json)
                    
                    if status_result.get("success"):
                        status_data = status_result.get("data", {})
                        current_status = status_data.get("status", "unknown")
                        
                        if status_data.get("is_completed"):
                            final_status = "completed"
                            workflow_results["final_status"] = "completed"
                            logger.info(f"ä»»åŠ¡å·²å®Œæˆ: {current_status}")
                            break
                        elif status_data.get("is_failed"):
                            final_status = "failed"
                            workflow_results["final_status"] = "failed"
                            logger.info(f"ä»»åŠ¡å¤±è´¥: {current_status}")
                            break
                        else:
                            # ä»»åŠ¡ä»åœ¨è¿è¡Œ
                            if ctx:
                                await ctx.session.send_log_message("info", f"ä»»åŠ¡çŠ¶æ€: {current_status}, å·²ç­‰å¾… {waited_time}s")
                    
                    await asyncio.sleep(check_interval)
                    waited_time += check_interval
                
                if waited_time >= max_wait_time:
                    workflow_results["final_status"] = "timeout"
                    final_status = "timeout"
                
                workflow_results["steps"].append({
                    "step": 3,
                    "name": "ç­‰å¾…ä»»åŠ¡å®Œæˆ",
                    "success": final_status == "completed",
                    "final_status": final_status,
                    "waited_time": waited_time
                })
            else:
                workflow_results["final_status"] = "submitted"
        else:
            workflow_results["final_status"] = "dag_created"
        
        total_execution_time = time.perf_counter() - workflow_start_time
        workflow_results["execution_times"]["total"] = total_execution_time
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        if workflow_results["final_status"] in ["completed", "submitted", "dag_created"]:
            result = Result.succ(
                data=workflow_results,
                msg=f"{operation}æˆåŠŸï¼ŒçŠ¶æ€: {workflow_results['final_status']}",
                operation=operation,
                execution_time=total_execution_time,
                api_endpoint="dag"
            )
        else:
            result = Result.failed(
                msg=f"{operation}å®Œæˆä½†çŠ¶æ€å¼‚å¸¸: {workflow_results['final_status']}",
                operation=operation
            )
            result.data = workflow_results
        
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶{total_execution_time:.2f}ç§’")
        
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            operation=operation
        )
        result.data = workflow_results
        return result.model_dump_json()

# ============ èµ„æºç®¡ç†å·²åˆ é™¤ ============

# ============ HTTPæœåŠ¡å™¨è®¾ç½® ============

def create_starlette_app(mcp_server: Server, *, debug: bool = False) -> Starlette:
    """åˆ›å»ºæ”¯æŒSSEçš„Starletteåº”ç”¨"""
    sse = SseServerTransport("/messages/")

    async def handle_sse(request: Request) -> None:
        async with sse.connect_sse(
            request.scope,
            request.receive,
            request._send,
        ) as (read_stream, write_stream):
            await mcp_server.run(
                read_stream,
                write_stream,
                mcp_server.create_initialization_options(),
            )

    async def handle_health(request: Request):
        return JSONResponse({
            "status": "healthy",
            "server": MCP_SERVER_NAME,
            "environment": "é¥æ„Ÿå¤§æ¥¼",
            "endpoints": {
                "sse": "/sse",
                "health": "/health",
                "messages": "/messages/"
            }
        })

    async def handle_info(request: Request):
        return JSONResponse({
            "server_name": MCP_SERVER_NAME,
            "environment": "é¥æ„Ÿå¤§æ¥¼",
            "version": "2.5.0-yaogan",
            "description": "å±±ä¸œè€•åœ°æµå‡ºåˆ†æMCPæœåŠ¡å™¨ - é¥æ„Ÿå¤§æ¥¼é€‚é…ç‰ˆ",
            "features": [
                "é¥æ„Ÿå¤§æ¥¼ç¯å¢ƒé€‚é…",
                "ç¯å¢ƒè¿é€šæ€§æ£€æŸ¥",
                "è‡ªåŠ¨Tokenåˆ·æ–°",
                "å¡å‘åˆ†æ", 
                "å±±ä¸œè€•åœ°æµå‡ºåˆ†æ",
                "å¤šçº¦æŸè€•åœ°æµå‡ºåˆ†æ",
                "å¤§æ•°æ®æŸ¥è¯¢",
                "DAGæ‰¹å¤„ç†å·¥ä½œæµ",
                "Livy Sparkä½œä¸šé›†æˆ",
                "MinIOå­˜å‚¨é›†æˆ",
                "SSEä¼ è¾“",
                "HTTP endpoints",
                "ç»“æ„åŒ–æ—¥å¿—",
                "æ€§èƒ½ç›‘æ§"
            ],
            "environment_config": {
                "frontend": OGE_FRONTEND_URL,
                "compute_cluster": COMPUTE_CLUSTER_MASTER,
                "livy_api": LIVY_API_BASE_URL,
                "minio": MINIO_ENDPOINT,
                "network": f"{YAOGAN_NETWORK_SEGMENT}.x"
            },
            "apis": {
                "intranet_api": INTRANET_API_BASE_URL,
                "dag_api": DAG_API_BASE_URL,
                "livy_api": LIVY_API_BASE_URL
            },
            "available_tools": [
                "check_yaogan_environment",
                "refresh_token",
                "check_token_status",
                "coverage_aspect_analysis", 
                "farmland_outflow",
                "run_big_query",
                "execute_code_to_dag",
                "submit_batch_task", 
                "query_task_status",
                "execute_dag_workflow"
            ],
            "token_management": {
                "type": "automatic",
                "description": "è‡ªåŠ¨æ£€æµ‹tokenè¿‡æœŸ(40003)å¹¶åˆ·æ–°ï¼Œä¹Ÿæ”¯æŒæ‰‹åŠ¨åˆ·æ–°",
                "auto_refresh": "æ£€æµ‹åˆ°40003é”™è¯¯æ—¶è‡ªåŠ¨åˆ·æ–°",
                "manual_refresh": "å¯ä½¿ç”¨refresh_tokenå·¥å…·æ‰‹åŠ¨åˆ·æ–°", 
                "credentials": f"{DEFAULT_USERNAME}/123456",
                "format": "Bearer <jwt_token>"
            },
            "big_data_components": {
                "spark": f"{COMPUTE_CLUSTER_MASTER}:9091",
                "hadoop": f"{COMPUTE_CLUSTER_MASTER}:8088",
                "hbase": f"{COMPUTE_CLUSTER_MASTER}:16010",
                "livy": LIVY_API_BASE_URL
            }
        })

    return Starlette(
        debug=debug,
        routes=[
            Route("/sse", endpoint=handle_sse),
            Route("/health", endpoint=handle_health),
            Route("/info", endpoint=handle_info),
            Mount("/messages/", app=sse.handle_post_message),
        ],
    )

# ============ ä¸»ç¨‹åº ============

async def run_stdio_server():
    """è¿è¡Œstdioæ¨¡å¼çš„æœåŠ¡å™¨"""
    logger.info("å¯åŠ¨é¥æ„Ÿå¤§æ¥¼MCPæœåŠ¡å™¨ (stdioæ¨¡å¼)...")
    
    try:
        from mcp import stdio_server
        
        async with stdio_server() as streams:
            await mcp._mcp_server.run(
                streams[0], streams[1], 
                mcp._mcp_server.create_initialization_options()
            )
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡å™¨...")
    except Exception as e:
        logger.error(f"æœåŠ¡å™¨è¿è¡Œå‡ºé”™: {e}")
    finally:
        logger.info("é¥æ„Ÿå¤§æ¥¼MCPæœåŠ¡å™¨å·²å…³é—­")

def run_http_server(host: str = "0.0.0.0", port: int = 8000):
    """è¿è¡ŒHTTPæ¨¡å¼çš„æœåŠ¡å™¨"""
    logger.info(f"å¯åŠ¨é¥æ„Ÿå¤§æ¥¼MCPæœåŠ¡å™¨ (HTTPæ¨¡å¼) - {host}:{port}")
    
    mcp_server = mcp._mcp_server
    starlette_app = create_starlette_app(mcp_server, debug=True)
    
    uvicorn.run(starlette_app, host=host, port=port)

# åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ æµ‹è¯•å·¥å…·

@mcp.tool()
async def test_dag_status_api(
    dag_id: str,
    ctx: Context = None
) -> str:
    """
    æµ‹è¯•DAGçŠ¶æ€æŸ¥è¯¢API - ç›´æ¥è°ƒç”¨ä¸ç»è¿‡å°è£…
    
    ç”¨äºè¯Šæ–­query_task_statusçš„é—®é¢˜
    """
    operation = "æµ‹è¯•DAGçŠ¶æ€API"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation} - DAG ID: {dag_id}")
        
        # æ„å»ºAPI URL
        api_url = f"{DAG_API_BASE_URL}/getState"
        params = {"dagId": dag_id}
        
        logger.info(f"æµ‹è¯•APIè°ƒç”¨: {api_url}?dagId={dag_id}")
        
        start_time = time.perf_counter()
        
        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.get(
                api_url,
                params=params,
                headers={
                    "Content-Type": "application/json",
                    "Authorization": INTRANET_AUTH_TOKEN
                }
            )
            
            execution_time = time.perf_counter() - start_time
            
            # è¯¦ç»†è®°å½•å“åº”ä¿¡æ¯
            response_info = {
                "status_code": response.status_code,
                "headers": dict(response.headers),
                "content_length": len(response.content),
                "text_preview": response.text[:200] if response.text else "Empty",
                "is_json": False,
                "execution_time": execution_time
            }
            
            # å°è¯•è§£æJSON
            json_data = None
            try:
                json_data = response.json()
                response_info["is_json"] = True
                response_info["json_data"] = json_data
            except Exception as e:
                response_info["json_error"] = str(e)
            
            result = Result.succ(
                data=response_info,
                msg=f"{operation}å®Œæˆ - çŠ¶æ€ç : {response.status_code}",
                operation=operation,
                execution_time=execution_time,
                api_endpoint="dag_test"
            )
            
            logger.info(f"{operation}å®Œæˆ - çŠ¶æ€ç : {response.status_code}, å†…å®¹é•¿åº¦: {len(response.content)}")
            
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆ")
        
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            operation=operation
        )
        return result.model_dump_json()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='é¥æ„Ÿå¤§æ¥¼MCPæœåŠ¡å™¨ - å±±ä¸œè€•åœ°æµå‡ºåˆ†æé€‚é…ç‰ˆ')
    parser.add_argument('--mode', choices=['stdio', 'http'], default='stdio', help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--host', default='0.0.0.0', help='HTTPæ¨¡å¼çš„ç»‘å®šåœ°å€')
    parser.add_argument('--port', type=int, default=8000, help='HTTPæ¨¡å¼çš„ç›‘å¬ç«¯å£')
    
    args = parser.parse_args()
    
    try:
        if args.mode == 'stdio':
            asyncio.run(run_stdio_server())
        else:
            run_http_server(args.host, args.port)
    except KeyboardInterrupt:
        print("\né¥æ„Ÿå¤§æ¥¼MCPæœåŠ¡å™¨å·²åœæ­¢")
    except Exception as e:
        logger.error(f"å¯åŠ¨å¤±è´¥: {e}") 