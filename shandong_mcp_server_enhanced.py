#!/usr/bin/env python3
"""
å±±ä¸œè€•åœ°æµå‡ºåˆ†æ MCPæœåŠ¡å™¨ - å¢å¼ºç‰ˆ
æ•´åˆFastMCPæ¡†æ¶ï¼Œæ”¯æŒHTTPå’Œstdioä¸¤ç§ä¼ è¾“æ–¹å¼
"""

import asyncio
import json
import logging
import httpx
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, TypeVar
from pydantic import BaseModel
from enum import IntEnum
from typing import Annotated
from pydantic import Field
import traceback

# MCP SDK å¯¼å…¥
try:
    from mcp.server.fastmcp import FastMCP, Context
    from mcp.server.sse import SseServerTransport
    from mcp.server import Server
    from starlette.applications import Starlette
    from starlette.requests import Request
    from starlette.responses import JSONResponse
    from starlette.routing import Mount, Route
    import uvicorn
    import argparse
except ImportError as e:
    print(f"Error importing enhanced MCP dependencies: {e}")
    print("Please install: pip install fastmcp starlette uvicorn")
    exit(1)

T = TypeVar("T")

# ============ é…ç½®éƒ¨åˆ† ============

# æœåŠ¡å™¨é…ç½®
MCP_SERVER_NAME = "shandong-cultivated-analysis-enhanced"

# APIé…ç½®
BASE_GATEWAY_URL = "http://172.20.70.142:16555/gateway"
OGE_API_BASE_URL = "http://172.30.22.116:16555/gateway/computation-api/process"
INTRANET_API_BASE_URL = BASE_GATEWAY_URL+"/computation-api/process"
INTRANET_AUTH_TOKEN = "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1aWQiOjMyNCwidXNlcl9uYW1lIjoiZWR1X2FkbWluIiwic2NvcGUiOlsid2ViIl0sImV4cCI6MTc1MDAwMzQ1NiwidXVpZCI6ImY5NTBjZmYyLTA3YzgtNDYxYS05YzI0LTkxNjJkNTllMmVmNiIsImF1dGhvcml0aWVzIjpbIkFETUlOSVNUUkFUT1JTIl0sImp0aSI6IkhrbG9YdDhiMTFmMDJXTFRON3pXc0FkVlk3TSIsImNsaWVudF9pZCI6InRlc3QiLCJ1c2VybmFtZSI6ImVkdV9hZG1pbiJ9.RAaOX2Bzqn0ys8ZpzlsYaVY6RQuYMNwzYXWcJ_9KD8U"
AUTH_TOKEN_URL = "http://172.20.70.141/api/oauth/token"
"http://172.20.70.142:16555/gateway/computation-api/vector/statistical/guoTuBianGeng"

# DAGæ‰¹å¤„ç†APIé…ç½®
DAG_API_BASE_URL = "http://172.20.70.141/api/oge-dag-22"
DEFAULT_USER_ID = "f950cff2-07c8-461a-9c24-9162d59e2ef6"
DEFAULT_USERNAME = "edu_admin"
CATALOG_URL   = "http://172.20.70.141/api/asset/batch-result/catalog"
# æ‰§è¡Œç»“æœä¸dagIdç»‘å®šï¼Œæ•°æ®æ’å…¥
INSERT_REPORT_URL = BASE_GATEWAY_URL+"/asset/algorithm-processing-result/insert"




# ============ å“åº”æ ¼å¼å®šä¹‰ ============

class RetCode(IntEnum):
    SUCCESS = 0
    FAILED = 1

class Result(BaseModel):
    success: bool = False
    code: Optional[int] = None
    msg: Optional[str] = None
    data: Optional[T] = None
    map_type:Optional[T] = None
    operation: Optional[str] = None
    execution_time: Optional[float] = None
    api_endpoint: Optional[str] = "oge"

    @classmethod
    def succ(cls, data: T = None, msg="æˆåŠŸ", operation=None, map_type=None, execution_time=None, api_endpoint="oge"):
        return cls(
            success=True, 
            code=RetCode.SUCCESS, 
            msg=msg, 
            data=data,
            map_type=map_type,
            operation=operation,
            execution_time=execution_time,
            api_endpoint=api_endpoint
        )

    @classmethod
    def failed(cls, code: int = RetCode.FAILED, msg="æ“ä½œå¤±è´¥", map_type=map_type, operation=None):
        return cls(success=False, code=code, msg=msg, map_type=map_type, operation=operation)

# ============ æ—¥å¿—é…ç½® ============

# def setup_logger(name: str = None, file: str = None, level=logging.INFO) -> logging.Logger:
#     """è®¾ç½®ç»“æ„åŒ–æ—¥å¿—"""
#     logger = logging.getLogger(name)
#     logger.propagate = False
#     logger.setLevel(level)
    
#     if logger.hasHandlers():
#         logger.handlers.clear()

#     formatter = logging.Formatter(
#         fmt='%(name)s - %(asctime)s - %(levelname)s - %(message)s',
#         datefmt='%Y-%m-%d %H:%M:%S'
#     )

#     # æ–‡ä»¶æ—¥å¿—
#     if file:
#         Path(file).parent.mkdir(parents=True, exist_ok=True)
#         file_handler = logging.FileHandler(filename=file, mode='a', encoding='utf-8')
#         file_handler.setFormatter(formatter)
#         file_handler.setLevel(level)
#         logger.addHandler(file_handler)

#     # æ§åˆ¶å°æ—¥å¿—
#     stream_handler = logging.StreamHandler()
#     stream_handler.setFormatter(formatter)
#     stream_handler.setLevel(level)
#     logger.addHandler(stream_handler)

#     return logger

def setup_mcp_logger(name, file, level=logging.INFO):
    logger = logging.getLogger(name)
    logger.setLevel(level)
    # ä¸€å®šè¦ Trueï¼Œè®©å®ƒå¾€ä¸Šå†’æ³¡åˆ° root
    logger.propagate = True

    # åªåœ¨ç¬¬ä¸€æ¬¡é…ç½®æ—¶åŠ  handler
    if not logger.handlers:
        formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        # æ–‡ä»¶
        if file:
            Path(file).parent.mkdir(parents=True, exist_ok=True)
            fh = logging.FileHandler(file, encoding="utf-8")
            fh.setFormatter(formatter)
            logger.addHandler(fh)
        # æ§åˆ¶å°
        sh = logging.StreamHandler()
        sh.setFormatter(formatter)
        logger.addHandler(sh)
    return logger

# åˆ›å»ºæ—¥å¿—å®ä¾‹
logger = setup_mcp_logger("shandong_mcp", "logs/shandong_mcp.log")
api_logger = setup_mcp_logger("shandong_api", "logs/api_calls.log")

# ============ FastMCPå®ä¾‹ ============

mcp = FastMCP(MCP_SERVER_NAME)

# ============ Tokenç®¡ç† ============

async def refresh_intranet_token() -> tuple[bool, str]:
    """è‡ªåŠ¨åˆ·æ–°å†…ç½‘token"""
    global INTRANET_AUTH_TOKEN
    
    try:
        logger.info("å¼€å§‹åˆ·æ–°å†…ç½‘token...")
        
        url = AUTH_TOKEN_URL
        
        params = {
            "scopes": "web",
            "client_secret": "123456",
            "client_id": "test",
            "grant_type": "password",
            "username": "edu_admin",
            "password": "123456"
        }
        
        body = {
            "username": "edu_admin",
            "password": "123456"
        }
        
        headers = {
            "Content-Type": "application/json"
        }
        
        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.post(url, params=params, json=body, headers=headers)
            
            if response.status_code == 200:
                data = response.json()
                
                if 'data' in data and 'token' in data['data']:
                    token = data['data']['token']
                    token_head = data['data'].get('tokenHead', 'Bearer').rstrip()  # å»æ‰å°¾éƒ¨ç©ºæ ¼
                    full_token = f"{token_head} {token}"
                    
                    # æ›´æ–°å…¨å±€token
                    INTRANET_AUTH_TOKEN = full_token
                    
                    logger.info(f"Tokenåˆ·æ–°æˆåŠŸ: {full_token[:50]}...")
                    logger.info(f"Tokenæ ¼å¼æ£€æŸ¥ - head: '{token_head}', length: {len(full_token)}")
                    return True, full_token
                else:
                    logger.error(f"Tokenå“åº”æ ¼å¼å¼‚å¸¸: {data}")
                    return False, f"Tokenå“åº”æ ¼å¼å¼‚å¸¸: {data}"
            else:
                error_msg = f"Tokenè·å–å¤±è´¥ - çŠ¶æ€ç : {response.status_code} - å“åº”: {response.text}"
                logger.error(error_msg)
                return False, error_msg
                
    except Exception as e:
        error_msg = f"Tokenåˆ·æ–°å¼‚å¸¸: {str(e)}"
        logger.error(error_msg)
        return False, error_msg

# ============ é€šç”¨APIè°ƒç”¨å‡½æ•° ============

async def call_api_with_timing(
    url: str,
    method: str = 'POST',
    params: dict | None = None, 
    json_data: dict = None,
    headers: dict = None,
    timeout: int = 120,
    auto_retry_on_token_expire: bool = True,
    use_intranet_token: bool = False
) -> tuple[dict, float]:
    """é€šç”¨APIè°ƒç”¨ï¼Œå¸¦æ€§èƒ½ç›‘æ§å’Œè‡ªåŠ¨tokenåˆ·æ–°"""
    global INTRANET_AUTH_TOKEN
    start_time = time.perf_counter()
    
    # å¦‚æœæŒ‡å®šä½¿ç”¨å†…ç½‘tokenï¼Œåˆ™åŠ¨æ€æ›´æ–°headers
    if use_intranet_token:
        if headers is None:
            headers = {"Content-Type": "application/json"}
        headers["Authorization"] = INTRANET_AUTH_TOKEN
        logger.info(f"ä½¿ç”¨å†…ç½‘token: {INTRANET_AUTH_TOKEN[:50]}...")
        logger.info(f"å®é™…å‘é€headers: {dict((k, v[:50] + '...' if k == 'Authorization' and len(v) > 50 else v) for k, v in headers.items())}")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨é‡è¯•
    should_auto_retry = (
        use_intranet_token and 
        auto_retry_on_token_expire
    )
    
    try:
        async with httpx.AsyncClient(timeout=timeout) as client:
            # å¤„ç†GETè¯·æ±‚çš„å‚æ•°
            if method.upper() == "GET":
                # headers.pop("Content-Type")
                response = await client.get(
                    # method=method.upper(),
                    url=url,
                    params=params,
                    headers=headers or {"Content-Type": "application/json"}
                )
            else:
                response = await client.request(
                    method=method.upper(),
                    url=url,
                    json=json_data,
                    headers=headers or {"Content-Type": "application/json"}
                )
            
            execution_time = time.perf_counter() - start_time
            
            if response.status_code == 200:
                # å®‰å…¨å¤„ç†JSONè§£æ
                response_text = response.text.strip()
                try:
                    result = response.json()
                    # result["info-url"] = str(response.url)
                except Exception as json_error:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬ä½œä¸ºç»“æœ
                    logger.info(f"å“åº”ä¸æ˜¯JSONæ ¼å¼ï¼Œä½œä¸ºçº¯æ–‡æœ¬å¤„ç†: {response_text[:100]}...")
                    # å¯¹äºDAGçŠ¶æ€æŸ¥è¯¢ï¼Œç›´æ¥è¿”å›æ–‡æœ¬çŠ¶æ€
                    if "/getState" in url:
                        result = response_text if response_text else "unknown"
                    else:
                        result = {
                            "raw_text": response_text,
                            "json_parse_error": str(json_error),
                            "content_type": response.headers.get("content-type", "unknown")
                        }
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºtokenè¿‡æœŸé”™è¯¯
                if (should_auto_retry and 
                    isinstance(result, dict) and 
                    result.get("code") == 40003):
                    
                    logger.warning("æ£€æµ‹åˆ°tokenè¿‡æœŸ(40003)ï¼Œå°è¯•è‡ªåŠ¨åˆ·æ–°...")
                    
                    # åˆ·æ–°token
                    success, new_token = await refresh_intranet_token()
                    
                    if success:
                        logger.info("Tokenåˆ·æ–°æˆåŠŸï¼Œé‡æ–°è°ƒç”¨API...")
                        
                        # ç¡®ä¿ä½¿ç”¨æ–°tokené‡æ–°æ„å»ºheaders
                        new_headers = None
                        if use_intranet_token:
                            new_headers = {
                                "Content-Type": "application/json",
                                "Authorization": new_token
                            }
                        
                        # é‡æ–°è°ƒç”¨APIï¼ˆé€’å½’ï¼Œä½†ç¦ç”¨è‡ªåŠ¨é‡è¯•é¿å…æ— é™å¾ªç¯ï¼‰
                        return await call_api_with_timing(
                            url=url,
                            method=method,
                            params=params,
                            json_data=json_data,
                            headers=new_headers,
                            timeout=timeout,
                            auto_retry_on_token_expire=False,  # ç¦ç”¨é‡è¯•é¿å…å¾ªç¯
                            use_intranet_token=False  # å·²ç»æ‰‹åŠ¨è®¾ç½®headersäº†ï¼Œä¸éœ€è¦å†æ¬¡è®¾ç½®
                        )
                    else:
                        logger.error(f"Tokenåˆ·æ–°å¤±è´¥: {new_token}")
                        api_logger.error(f"APIè°ƒç”¨å¤±è´¥(tokenåˆ·æ–°å¤±è´¥) - URL: {url}")
                        return {"error": f"Tokenè¿‡æœŸä¸”åˆ·æ–°å¤±è´¥: {new_token}", "code": 40003}, execution_time
                
                api_logger.info(f"APIè°ƒç”¨æˆåŠŸ - URL: {url} - è€—æ—¶: {execution_time:.4f}s")
                return result, execution_time
            elif response.status_code == 401 and should_auto_retry:
                # å¤„ç†HTTP 401çŠ¶æ€ç ï¼ˆè®¤è¯å¤±è´¥ï¼‰
                logger.warning("æ£€æµ‹åˆ°401çŠ¶æ€ç ï¼Œå°è¯•è‡ªåŠ¨åˆ·æ–°token...")
                
                # åˆ·æ–°token
                success, new_token = await refresh_intranet_token()
                
                if success:
                    logger.info("Tokenåˆ·æ–°æˆåŠŸï¼Œé‡æ–°è°ƒç”¨API...")
                    
                    # ç¡®ä¿ä½¿ç”¨æ–°tokené‡æ–°æ„å»ºheaders
                    new_headers = None
                    if use_intranet_token:
                        new_headers = {
                            "Content-Type": "application/json",
                            "Authorization": new_token
                        }
                    
                    # é‡æ–°è°ƒç”¨APIï¼ˆé€’å½’ï¼Œä½†ç¦ç”¨è‡ªåŠ¨é‡è¯•é¿å…æ— é™å¾ªç¯ï¼‰
                    return await call_api_with_timing(
                        url=url,
                        method=method,
                        params=params,
                        json_data=json_data,
                        headers=new_headers,
                        timeout=timeout,
                        auto_retry_on_token_expire=False,  # ç¦ç”¨é‡è¯•é¿å…å¾ªç¯
                        use_intranet_token=False  # å·²ç»æ‰‹åŠ¨è®¾ç½®headersäº†ï¼Œä¸éœ€è¦å†æ¬¡è®¾ç½®
                    )
                else:
                    logger.error(f"Tokenåˆ·æ–°å¤±è´¥: {new_token}")
                    api_logger.error(f"APIè°ƒç”¨å¤±è´¥(tokenåˆ·æ–°å¤±è´¥) - URL: {url}")
                    return {"error": f"401è®¤è¯å¤±è´¥ä¸”tokenåˆ·æ–°å¤±è´¥: {new_token}", "status_code": 401}, execution_time
            else:
                error_detail = f"APIè°ƒç”¨å¤±è´¥ - URL: {url} - çŠ¶æ€ç : {response.status_code} - è€—æ—¶: {execution_time:.4f}s"
                if response.status_code == 401:
                    current_token_preview = INTRANET_AUTH_TOKEN[:30] + "..." if INTRANET_AUTH_TOKEN else "None"
                    error_detail += f" - å½“å‰tokené¢„è§ˆ: {current_token_preview}"
                api_logger.error(error_detail)
                return {"error": response.text, "status_code": response.status_code}, execution_time
                
    except Exception as e:
        execution_time = time.perf_counter() - start_time
        api_logger.error(f"APIè°ƒç”¨å¼‚å¸¸ - URL: {url} - é”™è¯¯: {str(e)} - è€—æ—¶: {execution_time:.4f}s")
        return {"error": str(e)}, execution_time

# ============ å·¥å…·å®šä¹‰ ============

# @mcp.tool()
async def refresh_token(ctx: Context = None) -> str:
    """
    æ‰‹åŠ¨åˆ·æ–°å†…ç½‘è®¤è¯Token
    
    å½“é‡åˆ°tokenè¿‡æœŸé”™è¯¯(40003)æ—¶ï¼Œå¯ä»¥ä½¿ç”¨æ­¤å·¥å…·æ‰‹åŠ¨åˆ·æ–°tokenï¼Œå…¶ä»–é”™è¯¯æƒ…å†µï¼Œä¸tokenè¿‡æœŸå¼‚å¸¸æ— å…³çš„é—®é¢˜ï¼Œä¸éœ€è¦è°ƒå–
    """
    operation = "åˆ·æ–°Token"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"æ‰‹åŠ¨æ‰§è¡Œ{operation}")
        
        success, token_or_error = await refresh_intranet_token()
        
        if success:
            result = Result.succ(
                data={
                    "new_token": token_or_error[:50] + "...",  # åªæ˜¾ç¤ºå‰50ä¸ªå­—ç¬¦
                    "token_length": len(token_or_error),
                    "updated_at": time.strftime("%Y-%m-%d %H:%M:%S")
                },
                msg=f"{operation}æˆåŠŸ",
                map_type="refresh_token",
                operation=operation,
                api_endpoint="auth"
            )
            
            if ctx:
                await ctx.session.send_log_message("info", f"{operation}æˆåŠŸï¼Œæ–°tokenå·²æ›´æ–°")
        else:
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {token_or_error}",
                map_type="refresh_token",
                operation=operation
            )
            
            if ctx:
                await ctx.session.send_log_message("error", f"{operation}å¤±è´¥: {token_or_error}")
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ - æˆåŠŸ: {success}")
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            map_type="refresh_token",
            operation=operation
        )
        return result.model_dump_json()

# @mcp.tool()
async def check_token_status(ctx: Context = None) -> str:
    """
    æ£€æŸ¥å½“å‰å†…ç½‘è®¤è¯TokençŠ¶æ€,è¯·æ³¨æ„é™¤éæ˜¯tokenè¿‡æœŸå¼‚å¸¸çš„é”™è¯¯ï¼Œå¦åˆ™ä¸è¦è°ƒç”¨è¯¥å·¥å…·
    
    ç”¨äºè°ƒè¯•tokené—®é¢˜ï¼Œæ˜¾ç¤ºå½“å‰tokençš„ä¿¡æ¯
    """
    global INTRANET_AUTH_TOKEN
    operation = "æ£€æŸ¥TokençŠ¶æ€"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation}")
        
        if INTRANET_AUTH_TOKEN:
            # å°è¯•è§£æJWT tokençš„æœ‰æ•ˆæœŸï¼ˆå¦‚æœæ˜¯JWTæ ¼å¼ï¼‰
            token_info = {
                "has_token": True,
                "token_preview": INTRANET_AUTH_TOKEN[:50] + "...",
                "token_length": len(INTRANET_AUTH_TOKEN),
                "starts_with_bearer": INTRANET_AUTH_TOKEN.startswith("Bearer "),
                "has_double_space": "Bearer  " in INTRANET_AUTH_TOKEN,  # æ£€æµ‹åŒç©ºæ ¼é—®é¢˜
                "current_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "bearer_prefix_length": len(INTRANET_AUTH_TOKEN.split(' ')[0]) if ' ' in INTRANET_AUTH_TOKEN else 0
            }
            
            # å¦‚æœæ˜¯JWT tokenï¼Œå°è¯•è§£æè¿‡æœŸæ—¶é—´
            if "Bearer " in INTRANET_AUTH_TOKEN:
                jwt_part = INTRANET_AUTH_TOKEN.replace("Bearer ", "")
                try:
                    import base64
                    import json
                    # ç®€å•è§£æJWT payloadï¼ˆä¸éªŒè¯ç­¾åï¼‰
                    parts = jwt_part.split('.')
                    if len(parts) >= 2:
                        # æ·»åŠ paddingå¦‚æœéœ€è¦
                        payload = parts[1]
                        payload += '=' * (4 - len(payload) % 4)
                        decoded = base64.b64decode(payload)
                        payload_data = json.loads(decoded)
                        
                        if 'exp' in payload_data:
                            exp_time = payload_data['exp']
                            exp_readable = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(exp_time))
                            token_info["expires_at"] = exp_readable
                            token_info["expires_timestamp"] = exp_time
                            token_info["is_expired"] = time.time() > exp_time
                        
                        if 'user_name' in payload_data:
                            token_info["username"] = payload_data['user_name']
                            
                except Exception as e:
                    token_info["parse_error"] = str(e)
            
            result = Result.succ(
                data=token_info,
                msg=f"{operation}æˆåŠŸ",
                map_type="check_token_status",
                operation=operation,
                api_endpoint="debug"
            )
        else:
            result = Result.failed(
                msg=f"{operation}: å½“å‰æ²¡æœ‰token",
                map_type="check_token_status",
                operation=operation
            )
        
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆ")
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ")
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            map_type="check_token_status",
            operation=operation
        )
        return result.model_dump_json()

# @mcp.tool()
async def coverage_aspect_analysis(
    bbox: List[float],
    coverage_type: str = "Coverage",
    pretreatment: bool = True,
    product_value: str = "Platform:Product:ASTER_GDEM_DEM30",
    radius: int = 1,
    ctx: Context = None
) -> str:
    """
    å¡å‘åˆ†æ - åŸºäºDEMæ•°æ®è®¡ç®—å¡å‘ä¿¡æ¯
    
    Parameters:
    - bbox: è¾¹ç•Œæ¡†åæ ‡ [minLon, minLat, maxLon, maxLat]
    - coverage_type: è¦†ç›–ç±»å‹
    - pretreatment: æ˜¯å¦è¿›è¡Œé¢„å¤„ç†
    - product_value: äº§å“æ•°æ®æº
    - radius: è®¡ç®—åŠå¾„
    """
    operation = "å¡å‘åˆ†æ"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation} - è¾¹ç•Œæ¡†: {bbox}")
        
        # æ„å»ºç®—æ³•å‚æ•°
        algorithm_args = {
            "coverage": {
                "type": coverage_type,
                "pretreatment": pretreatment,
                "preParams": {"bbox": bbox},
                "value": [product_value]
            },
            "radius": radius
        }
        
        # è°ƒç”¨å†…ç½‘API
        api_payload = {
            "name": "Coverage.aspect",
            "args": algorithm_args,
            "dockerImageSource": "DOCKER_HUB"
        }
        
        api_result, execution_time = await call_api_with_timing(
            url=INTRANET_API_BASE_URL,
            json_data=api_payload,
            use_intranet_token=True
        )
        
        if "error" in api_result:
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {api_result.get('error')}",
                map_type="coverage_aspect_analysis",
                operation=operation
            )
        else:
            result = Result.succ(
                data=api_result,
                msg=f"{operation}æ‰§è¡ŒæˆåŠŸ",
                map_type="coverage_aspect_analysis",
                operation=operation,
                execution_time=execution_time,
                api_endpoint="intranet"
            )
        
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶{execution_time:.2f}ç§’")
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ - è€—æ—¶: {execution_time:.2f}ç§’")
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            map_type="coverage_aspect_analysis",
            operation=operation
        )
        return result.model_dump_json()

# spatial_intersection å·¥å…·å·²åˆ é™¤

# coverage_slope_analysis å·¥å…·å·²åˆ é™¤

# terrain_analysis_suite å·¥å…·å·²åˆ é™¤

# get_oauth_token å’Œ refresh_intranet_token å·¥å…·å·²åˆ é™¤

# @mcp.tool()
async def process_single_tool_testing(
    # region_id: str = "ASTGTM_N28E056",
    # product_id: str = "ASTER_GDEM_DEM30", 
    # center_lon: float = 56.25,
    # center_lat: float = 28.40,
    # zoom_level: int = 11,
    wait_for_completion: bool = False,  # é»˜è®¤ç«‹å³è¿”å›ï¼Œé¿å…è¶…æ—¶
    ctx: Context = None
) -> str:
    """
    è¯¥å·¥å…·ä¸ºï¼šæ•´ä½“æµç¨‹å•ä¸ªå·¥å…·æµ‹è¯•
    è¯¥å·¥å…·ä¸ºæµ‹è¯•ä½¿ç”¨ï¼Œé™¤éç”¨æˆ·æŒ‡å®šæµ‹è¯•çš„è¦æ±‚ï¼Œå¦åˆ™ä¸åº”è¯¥è¢«è°ƒç”¨ã€‚
    è¯¥å·¥å…·æµ‹è¯•çš„æ˜¯æŒ‰ç…§ä»£ç æäº¤çš„æ–¹å¼ï¼Œæ‰§è¡Œæˆä¸ªæµç¨‹ï¼Œè¿”å›æœ€ç»ˆçš„ç»“æœã€‚
    è¯¥æ–¹å¼çš„å·¥ä½œæµç¨‹ï¼š
    1. æäº¤ä»»åŠ¡ï¼ˆç«‹å³è¿”å›DAG IDï¼‰
    2. ä½¿ç”¨è¿”å›çš„DAG IDè°ƒç”¨ query_task_status æŸ¥è¯¢è¿›åº¦
    3. æ‰§è¡ŒçŠ¶æ€ä¸ºstartingä¸runningï¼Œè¦è°ƒç”¨query_task_status è½®è®­è¿›åº¦ï¼Œæ¯5ç§’ä¸€æ¬¡ï¼Œç›´åˆ°çŠ¶æ€æˆåŠŸæˆ–è€…è½®è®­æ¬¡æ•°è¶…è¿‡10æ¬¡
    
    Parameters:
    - wait_for_completion: æ˜¯å¦ç­‰å¾…ä»»åŠ¡å®Œæˆ (é»˜è®¤: Falseï¼Œç«‹å³è¿”å›é¿å…è¶…æ—¶)
    
    è¿”å›ä¿¡æ¯åŒ…å«ï¼š
    - ä»»åŠ¡çŠ¶æ€å’ŒDAG ID
    - ä¸‹ä¸€æ­¥æ“ä½œæŒ‡å¼•
    - æŸ¥è¯¢çŠ¶æ€çš„å…·ä½“å‚æ•°
    """
    operation = "å±±ä¸œè€•åœ°æµå‡ºåˆ†æ"
    region_id: str = "ASTGTM_N28E056",
    product_id: str = "ASTER_GDEM_DEM30", 
    center_lon: float = 56.25,
    center_lat: float = 28.40,
    zoom_level: int = 11,
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation} - åŒºåŸŸ: {region_id}, äº§å“: {product_id}")
        
        # æ„å»ºOGEä»£ç 
        oge_code = f"""import oge

oge.initialize()
service = oge.Service()

dem = service.getCoverage(coverageID="{region_id}", productID="{product_id}")
aspect = service.getProcess("Coverage.aspect").execute(dem, 1)

vis_params = {{"min": -1, "max": 1, "palette": ["#808080", "#949494", "#a9a9a9", "#bdbebd", "#d3d3d3","#e9e9e9"]}}
aspect.styles(vis_params).export("aspect")
oge.mapclient.centerMap({center_lon}, {center_lat}, {zoom_level})"""
        
        logger.info(f"ç”Ÿæˆçš„OGEä»£ç é•¿åº¦: {len(oge_code)} å­—ç¬¦")
        
        # è°ƒç”¨execute_dag_workflowæ‰§è¡Œå®Œæ•´å·¥ä½œæµ
        workflow_result = await execute_dag_workflow(
            code=oge_code,
            task_name="shandong_farmland_outflow_analysis",
            filename="shandong_aspect_analysis",
            auto_submit=True,
            wait_for_completion=wait_for_completion,
            check_interval=10,          # æ¯10ç§’è½®è¯¢ä¸€æ¬¡
            max_wait_time=1800,         # 30åˆ†é’Ÿè¶…æ—¶
            ctx=ctx
        )
        
        # è§£æworkflowç»“æœ
        import json
        workflow_data = json.loads(workflow_result)
        
        if workflow_data.get("success"):
            # æå–å…³é”®ä¿¡æ¯
            workflow_details = workflow_data.get("data", {})
            final_status = workflow_details.get("final_status", "unknown")
            
            result_data = {
                "region_id": region_id,
                "product_id": product_id,
                "analysis_type": "aspect_analysis",
                "map_center": {"lon": center_lon, "lat": center_lat, "zoom": zoom_level},
                "workflow_status": final_status,
                "execution_steps": workflow_details.get("steps", []),
                "execution_times": workflow_details.get("execution_times", {}),
                "dag_info": {
                    "dag_ids": workflow_details.get("dag_ids", []),
                    "primary_dag_id": workflow_details.get("dag_ids", ["unknown"])[0],
                    "task_name": "shandong_farmland_outflow_analysis"
                },
                "next_action": {
                    "tool_name": "query_task_status",
                    "parameters": {
                        "dag_id": workflow_details.get("dag_ids", ["unknown"])[0]
                    },
                    "description": "æŸ¥è¯¢ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€"
                } if final_status == "submitted" else None
            }
            
            if final_status == "completed":
                msg = f"{operation}æ‰§è¡ŒæˆåŠŸ - DEMå¡å‘åˆ†æå·²å®Œæˆå¹¶å¯è§†åŒ–"
            elif final_status == "submitted":
                primary_dag_id = workflow_details.get("dag_ids", ["unknown"])[0]
                msg = f"{operation}ä»»åŠ¡å·²æäº¤ - DAG ID: {primary_dag_id}\n" + \
                      f"ğŸ’¡ è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥è¯¢è¿›åº¦ï¼š\n" + \
                      f"query_task_status(dag_id=\"{primary_dag_id}\")"
            else:
                msg = f"{operation}æ‰§è¡Œå®Œæˆ - çŠ¶æ€: {final_status}"
            
            result = Result.succ(
                data=result_data,
                msg=msg,
                map_type="process_single_tool_testing",
                operation=operation,
                api_endpoint="dag_workflow"
            )
        else:
            # å·¥ä½œæµæ‰§è¡Œå¤±è´¥
            error_msg = workflow_data.get("msg", "å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
            final_status = "failed"
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {error_msg}",
                map_type="process_single_tool_testing",
                operation=operation
            )
            result.data = workflow_data.get("data")
        
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆ")
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ - æœ€ç»ˆçŠ¶æ€: {final_status}")
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            map_type="process_single_tool_testing",
            operation=operation
        )
        return result.model_dump_json()


# @mcp.tool()
async def shandong_farmland_vector_query_old(
    # administrative_division: Annotated[str,Field(description="è¦æŸ¥è¯¢çš„æ ‡å‡†åœ°åŒºåç§°",required = False)] = "é›ªé‡é•‡",
    administrative_divisions: Annotated[list[str], Field(description="è¦æŸ¥è¯¢çš„æ ‡å‡†åœ°åŒºåç§°åˆ—è¡¨", required=False)] = ["é›ªé‡é•‡"],
    year: Annotated[str,Field(description="è¦æŸ¥è¯¢çš„å¹´ä»½",required = False)] = "2023",
    ctx: Context = None
) -> str:
    """
    æ•°æ®é¢„å¤„ç†
    
    Return:
        åŒ…å«æ•°æ®æ˜¯å¦å­˜åœ¨çš„ä¿¡æ¯ä¸æŸ¥è¯¢è¯­å¥çš„ä¿¡æ¯åŒ…
    """
    operation = "è€•åœ°çŸ¢é‡æŸ¥è¯¢"
    query = "SELECT * FROM shp_guotubiangeng WHERE DLMC IN ('æ—±åœ°', 'æ°´æµ‡åœ°', 'æ°´ç”°')"
    ZLDWMC = ['ä¸‰æ§æ ‘æ‘', 'ä¸Šæ¸¸æ‘', 'ä¸Šç§‹æ—æ‘', 'ä¸œä¸‹æ¸¸æ‘', 'ä¸œå³ªæ‘', 'ä¸œå¼ æ‘', 'ä¸œæŠ¬å¤´æ‘', 'ä¸œæ ¾å®«æ‘', 'ä¸œç«™æ‘', 'å†¬æš–æ‘', 'åŒ—åŒç‹æ‘', 'åŒ—å³ªæ‘', 'åŒ—æ ¾å®«æ‘', 'åŒ—æ±Ÿæ°´æ‘', 'åŒ—ç™½åº§æ‘', 'åå±±æ‘', 'åå±±æ—åœº', 'å—åŒç‹æ‘', 'å—åœˆæ‘', 'å—å³ªæ‘', 'å—åµ¬çŸ³æ‘', 'å—æ ¾å®«æ‘', 'å—ç™½åº§æ‘', 'å•ç¥–æ³‰æ‘', 'å¤§å‚æ‘', 'å¤§å¯¨æ‘', 'å¤§ç½—åœˆæ‘', 'å¨˜å¨˜åº™æ‘', 'å­¦å±±æ‘', 'å®‰å­æ¹¾æ‘', 'å®˜æ­£æ‘', 'å¯Œå®¶åº„æ‘', 'å°æ¥¼æ‘', 'å²­ä¸œæ‘', 'æˆ¿å¹²æ‘', 'æœ›ç±³å°æ‘', 'æœ±å…¬æ³‰æ‘', 'æå®¶åº„æ‘', 'æç™½æ¨æ‘', 'æ –é¾™æ¹¾æ‘', 'æ¡¥å­æ‘', 'æ¯›å®¶æ—æ‘', 'ç‹‚å±±æ‘', 'ç‹è€æ‘', 'çŸ³å­å£æ‘', 'çŸ³æ³‰æ‘', 'çº¢å“¨å­æ‘', 'èƒ¡å¤šç½—æ‘', 'èƒ¡å®¶åº„æ‘', 'èˆ¹å‚æ‘', 'èŠ±å³ªæ‘', 'èœ‚çªæ‘', 'è¥¿ä¸‹æ¸¸æ‘', 'è¥¿å³ªæ²³åŒ—æ‘', 'è¥¿å³ªæ²³å—æ‘', 'è¥¿åµ¬çŸ³æ‘', 'è¥¿æŠ¬å¤´æ‘', 'è¥¿ç«™æ‘', 'é‚¢å®¶å³ªæ‘', 'é…‰å¡æ‘', 'é˜è€æ‘', 'é›ªé‡æ‘', 'é›ªé‡æ°´åº“', 'é’åˆåœˆæ‘', 'é©¬å®¶å³ªæ‘', 'é©¬éå±±æ—åœº', 'é²åœ°æ‘', 'é¹¿é‡æ‘', 'é»‘å±±æ‘', 'é¾™é©¬åº„æ‘']
    
    try:
        if ctx:
            if year and administrative_divisions:
                names = "ã€".join(administrative_divisions)
                await ctx.session.send_log_message("info", f"è·å–{year}å¹´{names}æ—è€•åœ°é€‚å®œæ€§è¯„ä»·å…³è”æ•°æ®")
            elif administrative_divisions:
                names = "ã€".join(administrative_divisions)
                await ctx.session.send_log_message("info", f"è·å–{names}æ—è€•åœ°é€‚å®œæ€§è¯„ä»·å…³è”æ•°æ®")
            elif year:
                await ctx.session.send_log_message("info", f"è·å–{year}å¹´æ—è€•åœ°é€‚å®œæ€§è¯„ä»·å…³è”æ•°æ®")
            else:
                await ctx.session.send_log_message("info", "è·å–æ—è€•åœ°é€‚å®œæ€§è¯„ä»·å…³è”æ•°æ®")

            await ctx.session.send_log_message("info", "è¿›è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥ä¸é¢„å¤„ç†")
        
        valid_divisions = set(ZLDWMC)
        removed_divisions = []
        valid_villages = []

        # åˆå§‹å»é‡
        administrative_divisions = list(set(administrative_divisions))
        api_result = {
            "status_code": 200,
            "msg": "",
            "data": {
                "query_sql": query,
                "valid_villages":valid_villages,
                "removed_divisions":removed_divisions
            }
        }

        if "é›ªé‡é•‡" in administrative_divisions:
            # è‹¥åŒ…å«â€œé›ªé‡é•‡â€ï¼Œåˆ™å¿½ç•¥å…¶ä»–æ‘åï¼Œä½†è®°å½•ä¸åˆæ³•çš„
            for name in administrative_divisions:
                if name != "é›ªé‡é•‡" and name not in valid_divisions:
                    removed_divisions.append(name)
            query = "SELECT * FROM shp_guotubiangeng WHERE DLMC IN ('æ—±åœ°', 'æ°´æµ‡åœ°', 'æ°´ç”°')"
        else:
            # ä»…ä¸ºæ‘çº§ï¼Œç­›é€‰æœ‰æ•ˆæ‘
            valid_villages = [name for name in administrative_divisions if name in valid_divisions]
            removed_divisions = [name for name in administrative_divisions if name not in valid_divisions]

            if valid_villages:
                division_sql = ", ".join(f"'{v}'" for v in valid_villages)
                query = f"SELECT * FROM shp_guotubiangeng WHERE DLMC IN ('æ—±åœ°', 'æ°´æµ‡åœ°', 'æ°´ç”°') AND ZLDWMC IN ({division_sql})"

        res_msg = f"æ•°æ®å­˜åœ¨ï¼Œå¯ä»¥æ‰§è¡Œè®¡ç®—"
        api_result["data"]["query_sql"]=query
        
        if "é›ªé‡é•‡" not in administrative_divisions and len(valid_villages) < 1 :
            api_result = {"info": f"æ•°æ®åº“å†…æ²¡æœ‰è¯¥å¹´ä»½æˆ–åœ°åŒºçš„æ•°æ®", "status_code": 200,"data":None}
            res_msg = f"æ•°æ®åº“å†…æ²¡æœ‰è¯¥å¹´ä»½/åœ°åŒºçš„æ•°æ®"
        if year != "2023":
            api_result = {"info": f"æ•°æ®åº“å†…æ²¡æœ‰è¯¥å¹´ä»½æˆ–åœ°åŒºçš„æ•°æ®", "status_code": 200,"data":None}
            res_msg = f"æ•°æ®åº“å†…æ²¡æœ‰è¯¥å¹´ä»½/åœ°åŒºçš„æ•°æ®"

        
        if "error" in api_result:
            error_detail = api_result.get('error', 'æœªçŸ¥é”™è¯¯')
            status_code = api_result.get('status_code', 'æœªçŸ¥çŠ¶æ€ç ')
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {error_detail} (çŠ¶æ€ç : {status_code})",
                map_type="shandong_farmland_vector_query",
                operation=operation
            )
            
        else:
            result = Result.succ(
                data=api_result,
                msg=f"{operation}æ‰§è¡ŒæˆåŠŸ,"+res_msg,
                map_type="shandong_farmland_vector_query",
                operation=operation,
                api_endpoint="intranet"
            )
        
        # if ctx:
        #     # await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶{execution_time:.2f}ç§’")
        #     await ctx.session.send_log_message("info", res_msg)
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ")
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            map_type="shandong_farmland_vector_query",
            operation=operation
        )
        return result.model_dump_json()


@mcp.tool()
async def shandong_farmland_vector_query(
    administrative_divisions: Annotated[list[str], Field(description="è¦æŸ¥è¯¢çš„æ ‡å‡†åœ°åŒºåç§°åˆ—è¡¨-list[str]", required=False)] = ["é›ªé‡é•‡"],
    year: Annotated[str,Field(description="è¦æŸ¥è¯¢çš„å¹´ä»½",required = False)] = "2023",
    ctx: Context = None
) -> str:
    """
    æ•°æ®é¢„å¤„ç†
    
    å¦‚æœæ•°æ®åº“æ— æ•°æ®çš„è¯ï¼Œä¸ç”¨å†æ¬¡è°ƒç”¨ï¼Œè¿”å›ç”¨æˆ·å³å¯
    Return:
        åŒ…å«æ•°æ®æ˜¯å¦å­˜åœ¨çš„ä¿¡æ¯ä¸æŸ¥è¯¢è¯­å¥çš„ä¿¡æ¯åŒ…,æŸ¥è¯¢æ— æ•°æ®ï¼Œä¸ä¼šè¿”å›æŸ¥è¯¢sql
    """
    operation = "è€•åœ°æ•°æ®æŸ¥è¯¢"
    DLMC_LIST = ["æ—±åœ°", "æ°´æµ‡åœ°", "æ°´ç”°"]
    ZLDWMC = []
    params = {"DLMC": ",".join(DLMC_LIST),"ZLDWMC":",".join(ZLDWMC)}
    vector_query_url = BASE_GATEWAY_URL+"/computation-api/vector/statistical/guoTuBianGeng"
    # params["DLMC"] = ",".join(DLMC_LIST)
    # params["ZLDWMC"] = ",".join(ZLDWMC)
    # if ZLDWMC:
    #     params["ZLDWMC"] = zldwmc
    
    try:
        # è°ƒç”¨é€šç”¨æ¥å£å‡½æ•°
        resp, _ = await call_api_with_timing(
            url=vector_query_url,
            method="GET",
            params=params,
            # å¦‚æœä½ ä»¬å†…éƒ¨éœ€è¦ tokenï¼Œå¯ä»¥åŠ  use_intranet_token=True
            use_intranet_token=True
        )
        
        MC_list = []
        # æ£€æŸ¥ codeã€æ‹¿åˆ° data
        if isinstance(resp, dict) and resp.get("code") == 20000:
            # ZLDWMC å°±æ˜¯ä¸€ä¸ª [{ "cnt": 3, "region_name": "...", "all_area": ... }, ...] çš„åˆ—è¡¨
            MC_list = resp.get("data", [])
        else:
            # æ ¹æ®å®é™…æƒ…å†µæŠ›é”™æˆ–è¿”å›ç©º
            raise RuntimeError(f"è°ƒç”¨å¤±è´¥ï¼š{resp}")

        if ctx:
            if year and administrative_divisions:
                names = "ã€".join(administrative_divisions)
                await ctx.session.send_log_message("info", f"è·å–{year}å¹´{names}æ—è€•åœ°é€‚å®œæ€§è¯„ä»·å…³è”æ•°æ®")
            elif administrative_divisions:
                names = "ã€".join(administrative_divisions)
                await ctx.session.send_log_message("info", f"è·å–{names}æ—è€•åœ°é€‚å®œæ€§è¯„ä»·å…³è”æ•°æ®")
            elif year:
                await ctx.session.send_log_message("info", f"è·å–{year}å¹´æ—è€•åœ°é€‚å®œæ€§è¯„ä»·å…³è”æ•°æ®")
            else:
                await ctx.session.send_log_message("info", "è·å–æ—è€•åœ°é€‚å®œæ€§è¯„ä»·å…³è”æ•°æ®")
        
        valid_divisions = set(item["region_name"] for item in MC_list)
        removed_divisions = []
        valid_villages = []

        query = ""
        # åˆå§‹å»é‡
        administrative_divisions = set(administrative_divisions)
        api_result = {
            "status_code": 200,
            "msg": "",
            "data": {
                "query_sql": query,
                "valid_villages":valid_villages,
                "removed_divisions":removed_divisions
            }
        }

        if "é›ªé‡é•‡" in administrative_divisions:
            # è‹¥åŒ…å«â€œé›ªé‡é•‡â€ï¼Œåˆ™å¿½ç•¥å…¶ä»–æ‘åï¼Œä½†è®°å½•ä¸åˆæ³•çš„
            for name in administrative_divisions:
                if name != "é›ªé‡é•‡" and name not in valid_divisions:
                    removed_divisions.append(name)
                else:
                    valid_villages.append(name)
            query = "SELECT * FROM shp_guotubiangeng WHERE DLMC IN ('æ—±åœ°', 'æ°´æµ‡åœ°', 'æ°´ç”°')"
            valid_villages.append("é›ªé‡é•‡")
        else:
            # ä»…ä¸ºæ‘çº§ï¼Œç­›é€‰æœ‰æ•ˆæ‘
            valid_villages.extend([name for name in administrative_divisions if name in valid_divisions])
            removed_divisions.extend([name for name in administrative_divisions if name not in valid_divisions])
            if valid_villages:
                division_sql = ", ".join(f"'{v}'" for v in valid_villages)
                query = f"SELECT * FROM shp_guotubiangeng WHERE DLMC IN ('æ—±åœ°', 'æ°´æµ‡åœ°', 'æ°´ç”°') AND ZLDWMC IN ({division_sql})"

        res_msg = f"æ•°æ®å­˜åœ¨ï¼Œå¯ä»¥æ‰§è¡Œè®¡ç®—"
        api_result["data"]["query_sql"]=query
        data_is_exist = True
        
        if "é›ªé‡é•‡" not in administrative_divisions and len(valid_villages) < 1 :
            api_result = {"info": f"æ•°æ®åº“å†…æ²¡æœ‰è¯¥å¹´ä»½æˆ–åœ°åŒºçš„æ•°æ®", "status_code": 200,"data":None}
            res_msg = "æ•°æ®åº“å†…æ²¡æœ‰è¯¥å¹´ä»½/åœ°åŒºçš„æ•°æ®"
            data_is_exist = False
        if year != "2023":
            api_result = {"info": f"æ•°æ®åº“å†…æ²¡æœ‰è¯¥å¹´ä»½æˆ–åœ°åŒºçš„æ•°æ®", "status_code": 200,"data":None}
            res_msg = "æ•°æ®åº“å†…æ²¡æœ‰è¯¥å¹´ä»½/åœ°åŒºçš„æ•°æ®"
            data_is_exist = False
        
        if "error" in api_result:
            error_detail = api_result.get('error', 'æœªçŸ¥é”™è¯¯')
            status_code = api_result.get('status_code', 'æœªçŸ¥çŠ¶æ€ç ')
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {error_detail} (çŠ¶æ€ç : {status_code})",
                map_type="shandong_farmland_vector_query",
                operation=operation
            )
            
        else:
            result = Result.succ(
                data=api_result,
                msg=f"{operation}æ‰§è¡ŒæˆåŠŸ,"+res_msg,
                map_type="shandong_farmland_vector_query",
                operation=operation,
                api_endpoint="intranet"
            )
            if ctx:
                if data_is_exist:
                    await ctx.session.send_log_message("info", "è¿›è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥ä¸é¢„å¤„ç†")
                else:
                    await ctx.session.send_log_message("info", "æ•°æ®åº“å†…æ²¡æœ‰è¯¥å¹´ä»½/åœ°åŒºçš„æ•°æ®")
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ")
        print(result)
        return result.model_dump_json()
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {e}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {e}",
            map_type="shandong_farmland_vector_query",
            operation=operation
        )
        return result.model_dump_json()

# è€•åœ°é€‚å®œæ€§åˆ†æ
@mcp.tool()
async def farmland_suitability_analysis(
    data_query_sql: Annotated[str,Field(description="æ•°æ®é¢„å¤„ç†çš„query_sql",required = True)],
    wait_for_completion: bool = True,
    ctx: Context = None
) -> str:
    """
    è€•åœ°åœ°å—åˆå¹¶

    """
    operation = "è€•åœ°æµå‡ºåˆ†æ"
    slope_threshold: int = 4           # å¡åº¦ç­‰çº§é˜ˆå€¼ï¼Œ4å¯¹åº”15åº¦
    fragment_area_threshold: float = 3333.3333  # ç»†ç¢åŒ–é¢ç§¯é˜ˆå€¼ï¼ˆ5äº©=3333.33å¹³æ–¹ç±³ï¼‰
    buffer_distance: float = 10.0     # ç¼“å†²åŒºè·ç¦»ï¼ˆç±³ï¼‰
    peripheral_area_threshold: float = 6666.6667  # å‘¨è¾¹é¢ç§¯é˜ˆå€¼ï¼ˆ10äº©ï¼‰
    # 630æ¼”ç¤ºï¼ŒåŠ å…¥ä¿¡æ¯
    additional_json_data = {
        "pre": [
            {
                "name": "ç”Ÿæ€ä¿æŠ¤çº¢çº¿",
                "type": "ecology",
                "url": "http://59.206.223.134:7000/service/xueye_bio_protected_boundary?type=wvts&tablename=xueye_bio_protected_boundary&z={z}&x={x}&y={y}",
                "detailmeta": "{center: [121.709337,37.308754],zoom:12}"
            },
            {
                "name": "åŸé•‡å¼€å‘è¾¹ç•Œ",
                "type": "town",
                "url": "http://59.206.223.134:7000/service/xueye_urban_boundary?type=wvts&tablename=xueye_urban_boundary&z={z}&x={x}&y={y}",
                "detailmeta": "{center: [121.709337,37.308754],zoom:12}"
            },
            {
                "name": "è€•åœ°åœ°å—æ•°æ®",
                "type": "farmland",
                "url": "http://59.206.223.134:7000/service/ogeArable?type=wvts&tablename=ogeArable&z={z}&x={x}&y={y}",
                "detailmeta": "{center: [121.709337,37.308754],zoom:12}"
            },
            {
                "name": "å¤§äº15åº¦çš„å¡åº¦",
                "type": "slope",
                "url": "http://59.206.223.134:7000/service/xueye_slope_boundary?type=wvts&tablename=xueye_slope_boundary&z={z}&x={x}&y={y}",
                "detailmeta": "{center: [121.709337,37.308754],zoom:12}"
            }
        ],
        "aft": [
            {
                "name": "å¡åº¦å¤§äº15åº¦è€•åœ°",
                "type": "slope",
                "processId": "f950cff2-07c8-461a-9c24-9162d59e2ef6_1749970088021_3012"
            },
            {
                "name": "ç»†ç¢åŒ–è€•åœ°",
                "type": "fragmented",
                "processId": "f950cff2-07c8-461a-9c24-9162d59e2ef6_1749970088021_3012"
            },
            {
                "name": "åœ¨ç”Ÿæ€ä¿æŠ¤çº¢çº¿å†…",
                "type": "ecology",
                "processId": "f950cff2-07c8-461a-9c24-9162d59e2ef6_1749970088021_3012"
            },
            {
                "name": "åœ¨åŸé•‡å¼€å‘è¾¹ç•Œå†…",
                "type": "urban",
                "processId": "f950cff2-07c8-461a-9c24-9162d59e2ef6_1749970088021_3012"
            }
        ]
    }
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", "è¿›è¡Œå·²æå–è€•åœ°åœ°å—åˆå¹¶")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation} - å¡åº¦é˜ˆå€¼: {slope_threshold}, é¢ç§¯é˜ˆå€¼: {fragment_area_threshold}")
        
        # æ„å»ºOGEä»£ç 
        oge_code = f"""import oge
oge.initialize()

service = oge.Service.initialize()
query = r"{data_query_sql}"
cultivated = service.getProcess("FeatureCollection.runBigQuery").execute(query, "geom") #è€•åœ°
cultivated_bounds = service.getProcess("FeatureCollection.bounds").execute(cultivated)
# é‡ç‚¹ç®¡æ§åŒºåŸŸ å¤§äº15åº¦è€•åœ° å‡ ä½•
slope = service.getFeatureCollection("shp_podu") #å¡åº¦ GCS_China_Geodetic_Coordinate_System_2000
slope_morethan15_ = service.getProcess("FeatureCollection.filterMetadata").execute(slope, "pdjb", "greater_than", 4) #è¶…è¿‡15åº¦çš„è€•åœ°
slope_morethan15 = service.getProcess("FeatureCollection.reproject").execute(slope_morethan15_, "EPSG:4527")
slope_extent = service.getProcess("FeatureCollection.filterBounds").execute(slope_morethan15, cultivated_bounds)
urban_ = service.getFeatureCollection("shp_chengzhenkaifa") #åŸé•‡å¼€å‘è¾¹ç•Œ
urban = service.getProcess("FeatureCollection.reproject").execute(urban_, "EPSG:4527")
ecology_ = service.getFeatureCollection("shp_shengtaibaohu") #ç”Ÿæ€ä¿æŠ¤çº¢çº¿
ecology = service.getProcess("FeatureCollection.reproject").execute(ecology_, "EPSG:4527")

# cultivated_protected = service.getProcess("FeatureCollection.reproject").execute(cultivated, "EPSG:4527")

urban_intersection = service.getProcess("FeatureCollection.intersection").execute(cultivated, urban) #æµå‡º1
urban_erase = service.getProcess("FeatureCollection.erase").execute(cultivated, urban)
ecology_intersection = service.getProcess("FeatureCollection.intersection").execute(urban_erase, ecology) ##æµå‡º3
ecology_erase = service.getProcess("FeatureCollection.erase").execute(urban_erase, ecology)
slope_intersection = service.getProcess("FeatureCollection.intersection").execute(ecology_erase, slope_extent) #æµå‡º4
slope_erase = service.getProcess("FeatureCollection.erase").execute(ecology_erase, slope_extent)

#ç­›é€‰ç»†ç¢åŒ–è€•åœ°
cultivated1_area = service.getProcess("FeatureCollection.area").execute(slope_erase) #å¢åŠ areaå­—æ®µ
cultivated1_lessthan5 = service.getProcess("FeatureCollection.filterMetadata").execute(cultivated1_area, "area", "less_than", 3333.3333)
cultivated1_buffer = service.getProcess("FeatureCollection.buffer").execute(cultivated1_lessthan5, 10)
cultivated1_join = service.getProcess("FeatureCollection.spatialJoinOneToOne").execute(cultivated1_buffer, cultivated1_area, "buffer", "geom", True, "Intersects", ["area"], ["sum"])
cultivated1_subtract = service.getProcess("FeatureCollection.subtract").execute(cultivated1_join, "area_sum", "area", "area_peri")
deprecated1 = service.getProcess("FeatureCollection.filterMetadata").execute(cultivated1_subtract, "area_peri", "less_than", "6666.6667") #æµå‡º5

urban_intersection_reason = service.getProcess("FeatureCollection.constantColumn").execute(urban_intersection, "reason", "urban")
ecology_intersection_reason = service.getProcess("FeatureCollection.constantColumn").execute(ecology_intersection, "reason", "ecology")
slope_intersection_reason = service.getProcess("FeatureCollection.constantColumn").execute(slope_intersection, "reason", "slope")
deprecated1_reason = service.getProcess("FeatureCollection.constantColumn").execute(deprecated1, "reason", "fragmented")

deprecated = service.getProcess("FeatureCollection.mergeAll").execute([urban_intersection_reason,ecology_intersection_reason,slope_intersection_reason,deprecated1_reason]) #éœ€è¦æµå‡ºçš„è€•åœ°
deprecated_area = service.getProcess("FeatureCollection.area").execute(deprecated)
deprecated_CGCS2000 = service.getProcess("FeatureCollection.reproject").execute(deprecated_area, "EPSG:4490")
deprecated_CGCS2000.export("cultivated_protected")"""  
        
    
        logger.info(f"ç”Ÿæˆçš„OGEä»£ç é•¿åº¦: {len(oge_code)} å­—ç¬¦")
        
        # è°ƒç”¨execute_dag_workflowæ‰§è¡Œå®Œæ•´å·¥ä½œæµ
        res_filename = "å¤§æ¨¡å‹farmland_outflow_result"+str(time.time())
        workflow_result = await execute_dag_workflow(
            code=oge_code,
            task_name=res_filename,
            filename=res_filename,
            auto_submit=True,
            wait_for_completion=wait_for_completion,
            format="geojson",
            check_interval=10,          # æ¯10ç§’è½®è¯¢ä¸€æ¬¡
            max_wait_time=1800,         # 30åˆ†é’Ÿè¶…æ—¶
            ctx=ctx
        )
        
        # è§£æworkflowç»“æœ
        import json
        workflow_data = json.loads(workflow_result)
        
        if workflow_data.get("success"):
            # æå–å…³é”®ä¿¡æ¯
            workflow_details = workflow_data.get("data", {})
            final_status = workflow_details.get("final_status", "unknown")

            result_data = {
                "analysis_type": "farmland_outflow_analysis",
                "workflow_status": final_status,
                "dag_id": workflow_details.get("dag_ids", ["unknown"])[0]
                # "outflow_categories": [
                #     {"type": "urban", "description": "åŸé•‡å¼€å‘è¾¹ç•Œå†…è€•åœ°"},
                #     {"type": "nature", "description": "è‡ªç„¶ä¿æŠ¤åœ°å†…è€•åœ°"},
                #     {"type": "ecology", "description": "ç”Ÿæ€ä¿æŠ¤çº¢çº¿å†…è€•åœ°"},
                #     {"type": "slope",   "description": f"å¡åº¦å¤§äº{slope_threshold}çº§çš„è€•åœ°"},
                #     {"type": "fragmented", "description": f"ç»†ç¢åŒ–è€•åœ°ï¼ˆ<{fragment_area_threshold/666.67:.1f}äº©ï¼‰"}
                # ]
            }
            if final_status == "completed":
                msg = f"{operation}æ‰§è¡ŒæˆåŠŸ - è€•åœ°æµå‡ºåˆ†æå·²å®Œæˆ"
                # æ’å…¥ç»“æœä¿¡æ¯åˆ°æ•°æ®åº“ï¼Œç»‘å®šrecordIdä¸ç»“æœæ–‡ä»¶ï¼Œä»¥ä¾¿å¯ä»¥é€šè¿‡å¦ä¸€ä¸ªæ¥å£æŸ¥æ‰¾
                workflow_report_payload = {
                    "uid": 324,
                    "algorithmName": "å¤§æ¨¡å‹è€•åœ°é€‚å®œæ€§åˆ†æå•å·¥å…·",
                    "algorithmResultName": f"{res_filename}.geojson",  # æ³¨æ„filenameåº”ä¸å¸¦æ‰©å±•å
                    "resultStatus": 1,
                    "resultFileStatus": 1,
                    "processingTime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),  # æˆ–ä¼ å…¥å€¼
                    "recordId": workflow_details.get("dag_ids", ["unknown"])[0],  # æˆ–æ›¿æ¢ä¸ºè‡ªå®šä¹‰recordId
                    "filePath": f"oge-user/f950cff2-07c8-461a-9c24-9162d59e2ef6/result/"
                }

                # è°ƒç”¨ç»Ÿä¸€å°è£…å‡½æ•°,ç»‘å®šidä¸ç»“æœæ–‡ä»¶
                workflow_report_result, _ = await call_api_with_timing(
                    url=INSERT_REPORT_URL,
                    json_data=workflow_report_payload,
                    use_intranet_token=True  # å…³é”®ç‚¹ï¼šå¼€å¯å†…ç½‘tokenè‡ªåŠ¨å¤„ç†
                )

                # ä½ å¯ä»¥æ ¹æ®è¿”å›åšé¢å¤–å¤„ç†
                if isinstance(workflow_report_result, dict) and workflow_report_result.get("code") == 200:
                    logger.info("ç®—æ³•å¤„ç†ç»“æœæˆåŠŸä¸ŠæŠ¥ç»‘å®šprocessId")
                else:
                    logger.warning(f"ä¸ŠæŠ¥å¤±è´¥ï¼Œå“åº”: {workflow_report_result}")

            elif final_status == "submitted":
                primary_dag_id = workflow_details.get("dag_ids", ["unknown"])[0]
                msg = f"{operation}ä»»åŠ¡å·²æäº¤ - dagId/processId/recordId: {primary_dag_id}\n" + \
                      f"è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥è¯¢è¿›åº¦ï¼š\n" + \
                      f"query_task_status(recordId=\"{primary_dag_id}\")\n" + \
                      f"åˆ†æå‚æ•°ï¼šå¡åº¦é˜ˆå€¼{slope_threshold}çº§ï¼Œé¢ç§¯é˜ˆå€¼{fragment_area_threshold/666.67:.1f}äº©"
            else:
                msg = f"{operation}æ‰§è¡Œå®Œæˆ - çŠ¶æ€: {final_status}"
            
            result = Result.succ(
                data=result_data,
                msg=msg,
                map_type="farmland_suitability_analysis",
                operation=operation,
                api_endpoint="dag_workflow"
            )
        else:
            # å·¥ä½œæµæ‰§è¡Œå¤±è´¥
            workflow_details = workflow_data.get("data", {})
            error_msg = workflow_data.get("msg", "å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
            final_status = "failed"
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {error_msg}",
                map_type="farmland_suitability_analysis",
                operation=operation
            )
            result.data = workflow_data.get("data")
        additional_json_data = update_process_id(additional_json_data,workflow_details.get("dag_ids", ["unknown"])[0])
        if ctx:
            await ctx.session.send_log_message("info", "è€•åœ°åœ°å—åˆå¹¶å®Œæˆ")
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ - æœ€ç»ˆçŠ¶æ€: {final_status}")
        # æ¼”ç¤ºï¼Œéœ€è¦åŠ è¿›å»çš„æ•°æ®
        
        result.data = {**(result.data or {}), **additional_json_data}
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            map_type="farmland_suitability_analysis",
            operation=operation
        )
        result.data = {**(result.data or {}), **additional_json_data}
        return result.model_dump_json()


# @mcp.tool()
async def run_big_query(
    # query: str,
    # geometry_column: str = "geom",
    ctx: Context = None
) -> str:
    """
    æŸ¥è¯¢å±±ä¸œçœè€•åœ°çŸ¢é‡,åªä¼šè¿”å›æ•°æ®çš„æ ‡è¯†ï¼Œé€šè¿‡æ ‡è¯†åç»­å¯ä»¥è®¿é—®ç»“æœæ•°æ®
    
    Parameters:
        æ— å‚æ•°
    """
    operation = "å¤§æ•°æ®æŸ¥è¯¢"
    query = "SELECT * FROM shp_guotubiangeng WHERE DLMC IN ('æ—±åœ°', 'æ°´æµ‡åœ°', 'æ°´ç”°')"
    geometry_column = "geom"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation} - æŸ¥è¯¢: {query[:100]}...")
        
        # æ„å»ºç®—æ³•å‚æ•°
        algorithm_args = {
            "query": query,
            "geometryColumn": geometry_column
        }
        
        # è°ƒç”¨å†…ç½‘API
        api_payload = {
            "name": "FeatureCollection.runBigQuery",
            "args": algorithm_args,
            "dockerImageSource": "DOCKER_HUB"
        }
        
        api_result, execution_time = await call_api_with_timing(
            url=INTRANET_API_BASE_URL,
            json_data=api_payload,
            use_intranet_token=True
        )
        
        if "error" in api_result:
            error_detail = api_result.get('error', 'æœªçŸ¥é”™è¯¯')
            status_code = api_result.get('status_code', 'æœªçŸ¥çŠ¶æ€ç ')
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {error_detail} (çŠ¶æ€ç : {status_code})",
                map_type="run_big_query",
                operation=operation
            )
        else:
            result = Result.succ(
                data=api_result,
                msg=f"{operation}æ‰§è¡ŒæˆåŠŸ",
                map_type="run_big_query",
                operation=operation,
                execution_time=execution_time,
                api_endpoint="intranet"
            )
        
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶{execution_time:.2f}ç§’")
        
        logger.info(f"{operation}æ‰§è¡Œå®Œæˆ - è€—æ—¶: {execution_time:.2f}ç§’")
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            map_type="run_big_query",
            operation=operation
        )
        return result.model_dump_json()


# ============ DAGæ‰¹å¤„ç†å·¥å…· ============

# @mcp.tool()
async def execute_code_to_dag(
    code: str,
    user_id: str = DEFAULT_USER_ID,
    sample_name: str = "",
    auth_token: str = None,
    ctx: Context = None
) -> str:
    """
    å°†ä»£ç è½¬åŒ–ä¸ºDAGç”Ÿæˆä»»åŠ¡
    
    Parameters:
    - code: è¦æ‰§è¡Œçš„OGEä»£ç 
    - user_id: ç”¨æˆ·UUID
    - sample_name: ç¤ºä¾‹ä»£ç åç§°ï¼ˆå¯ä¸ºç©ºï¼‰
    - auth_token: è®¤è¯Tokenï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€Tokenï¼‰
    """
    operation = "ä»£ç è½¬DAGä»»åŠ¡"
    
    try:
        # if ctx:
        #     await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation}")
        
        # æ„å»ºAPI URL
        api_url = f"{DAG_API_BASE_URL}/executeCode"
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        request_data = {
            "code": code,
            "userId": user_id,
            "sampleName": sample_name
        }
        
        # å‡†å¤‡è®¤è¯
        use_custom_token = bool(auth_token)
        final_headers = None
        
        if use_custom_token:
            if not auth_token.startswith("Bearer "):
                auth_token = f"Bearer {auth_token}"
            final_headers = {
                "Content-Type": "application/json",
                "Authorization": auth_token
            }
        
        logger.info(f"è°ƒç”¨API: {api_url}")
        logger.info(f"è¯·æ±‚æ•°æ®: userId={user_id}, sampleName={sample_name}")
        
        # è°ƒç”¨API
        api_result, execution_time = await call_api_with_timing(
            url=api_url,
            method="POST",
            json_data=request_data,
            headers=final_headers,
            timeout=300,     # 5åˆ†é’Ÿè¶…æ—¶ï¼ŒDAGåˆ›å»ºå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
            use_intranet_token=not use_custom_token
        )
        
        if "error" not in api_result:
            # æå–DAGä¿¡æ¯
            dags = api_result.get("dags", {})
            space_params = api_result.get("spaceParams", {})
            log_info = api_result.get("log", "")
            
            # æå–DAG ID
            dag_ids = []
            for key, value in dags.items():
                if isinstance(value, dict):
                    dag_ids.append(key)
                elif isinstance(value, str):
                    dag_ids.append(value)
            
            result_data = {
                "dags": dags,
                "dag_ids": dag_ids,
                "space_params": space_params,
                "log": log_info,
                "user_id": user_id,
                "sample_name": sample_name
            }
            
            result = Result.succ(
                data=result_data,
                msg=f"{operation}æˆåŠŸï¼Œç”Ÿæˆäº†{len(dag_ids)}ä¸ªDAGä»»åŠ¡",
                operation=operation,
                map_type="execute_code_to_dag",
                execution_time=execution_time,
                api_endpoint="dag"
            )
            
            logger.info(f"{operation}æˆåŠŸ - ç”ŸæˆDAGæ•°é‡: {len(dag_ids)}")
            
        else:
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {api_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                map_type="execute_code_to_dag",
                operation=operation
            )
        
        # if ctx:
        #     await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶{execution_time:.2f}ç§’")
        
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            map_type="execute_code_to_dag",
            operation=operation
        )
        return result.model_dump_json()

# @mcp.tool()
async def submit_batch_task(
    dag_id: str,
    task_name: str = None,
    filename: str = None,
    crs: str = "EPSG:4326",
    scale: str = "1000",
    format: str = "geojson",
    username: str = DEFAULT_USERNAME,
    script: str = "",
    auth_token: str = None,
    ctx: Context = None
) -> str:
    """
    æäº¤æ‰¹å¤„ç†ä»»åŠ¡è¿è¡Œ
    
    Parameters:
    - dag_id: DAGä»»åŠ¡ID
    - task_name: ä»»åŠ¡åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰
    - filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰
    - crs: åæ ‡å‚è€ƒç³»ç»Ÿ
    - scale: æ¯”ä¾‹å°º
    - format: è¾“å‡ºæ ¼å¼
    - username: ç”¨æˆ·å
    - script: è„šæœ¬ä»£ç 
    - auth_token: è®¤è¯Tokenï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€Tokenï¼‰
    """
    operation = "æäº¤æ‰¹å¤„ç†ä»»åŠ¡"
    
    try:
        # if ctx:
        #     await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation} - DAG ID: {dag_id}")
        
        # æ„å»ºAPI URL
        api_url = f"{DAG_API_BASE_URL}/addTaskRecord"
        
        # ç”Ÿæˆé»˜è®¤ä»»åŠ¡åå’Œæ–‡ä»¶åï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if not task_name:
            timestamp = time.strftime("%Y_%m_%d_%H_%M_%S")
            task_name = f"task_{timestamp}"
            
        if not filename:
            timestamp = time.strftime("%Y_%m_%d_%H_%M_%S")
            filename = f"file_{timestamp}"
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        request_data = {
            "taskName": task_name,
            "crs": crs,
            "scale": scale,
            "filename": filename,
            "format": format,
            "id": dag_id,
            "userName": username,
            "script": script
        }
        
        # å‡†å¤‡è®¤è¯
        use_custom_token = bool(auth_token)
        final_headers = None
        
        if use_custom_token:
            if not auth_token.startswith("Bearer "):
                auth_token = f"Bearer {auth_token}"
            final_headers = {
                "Content-Type": "application/json",
                "Authorization": auth_token
            }
        
        logger.info(f"è°ƒç”¨API: {api_url}")
        logger.info(f"è¯·æ±‚æ•°æ®: taskName={task_name}, dagId={dag_id}")
        
        # è°ƒç”¨API
        api_result, execution_time = await call_api_with_timing(
            url=api_url,
            method="POST",
            json_data=request_data,
            headers=final_headers,
            timeout=300,     # 5åˆ†é’Ÿè¶…æ—¶ï¼Œä»»åŠ¡æäº¤å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
            use_intranet_token=not use_custom_token
        )
        
        if "error" not in api_result:
            # æ£€æŸ¥APIå“åº”æ ¼å¼
            if api_result.get("code") == 200:
                task_data = api_result.get("data", {})
                
                result_data = {
                    "batch_session_id": task_data.get("batchSessionId"),
                    "task_id": task_data.get("id"),
                    "dag_id": task_data.get("dagId"),
                    "task_name": task_data.get("taskName"),
                    "state": task_data.get("state"),
                    "filename": task_data.get("filename"),
                    "format": task_data.get("format"),
                    "scale": task_data.get("scale"),
                    "crs": task_data.get("crs"),
                    "user_id": task_data.get("userId"),
                    "username": task_data.get("userName"),
                    "folder": task_data.get("folder"),
                    "api_response": api_result
                }
                
                result = Result.succ(
                    data=result_data,
                    msg=f"{operation}æˆåŠŸï¼Œä»»åŠ¡çŠ¶æ€: {task_data.get('state', 'unknown')}",
                    operation=operation,
                    map_type="submit_batch_task",
                    execution_time=execution_time,
                    api_endpoint="dag"
                )
                
                logger.info(f"{operation}æˆåŠŸ - ä»»åŠ¡ID: {task_data.get('id')}, çŠ¶æ€: {task_data.get('state')}")
                
            else:
                result = Result.failed(
                    msg=f"{operation}å¤±è´¥: {api_result.get('msg', 'æœªçŸ¥é”™è¯¯')}",
                    map_type="submit_batch_task",
                    operation=operation
                )
        else:
            result = Result.failed(
                msg=f"{operation}å¤±è´¥: {api_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                map_type="submit_batch_task",
                operation=operation
            )
        
        # if ctx:
        #     await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶{execution_time:.2f}ç§’")
        
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            map_type="submit_batch_task",
            operation=operation
        )
        return result.model_dump_json()


# @mcp.tool() 
async def old_query_task_status(
    dag_id: str,
    auth_token: str = None,
    ctx: Context = None
) -> str:
    """
    æŸ¥è¯¢æ‰¹å¤„ç†ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€ï¼Œå¹¶é€šè¿‡ç»“æœç›®å½•æ¥å£æ ¡éªŒæœ€ç»ˆç»“æœçŠ¶æ€
    
    Parameters:
    - dag_id: DAG ä»»åŠ¡ ID
    - auth_token: è®¤è¯ Tokenï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€ Tokenï¼‰
    """
    operation = "æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"
    RESULT_CATALOG_URL = "http://172.20.70.141/api/asset/batch-result/catalog"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ {operation}...")
        logger.info(f"å¼€å§‹æ‰§è¡Œ {operation} - DAG ID: {dag_id}")
        
        # æ„å»º DAG çŠ¶æ€æ¥å£ URL
        api_url = f"{DAG_API_BASE_URL}/getState"
        
        # å‡†å¤‡è®¤è¯å¤´
        use_custom_token = bool(auth_token)
        final_headers = None
        if use_custom_token:
            if not auth_token.startswith("Bearer "):
                auth_token = f"Bearer {auth_token}"
            final_headers = {
                "Content-Type": "application/json",
                "Authorization": auth_token
            }
        
        params = {"dagId": dag_id}
        
        # è°ƒç”¨ DAG çŠ¶æ€æ¥å£
        if use_custom_token:
            start_time = time.perf_counter()
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.get(api_url, params=params, headers=final_headers)
            execution_time = time.perf_counter() - start_time
            
            if response.status_code == 200:
                # è§£æè¿”å›çš„çŠ¶æ€ï¼ˆå¯èƒ½æ˜¯ JSONï¼Œä¹Ÿå¯èƒ½æ˜¯çº¯æ–‡æœ¬ï¼‰
                text = response.text.strip()
                if not text:
                    status_str = "unknown"
                    raw = None
                else:
                    # è§£æ JSON æˆ–å½“ä½œçº¯æ–‡æœ¬
                    try:
                        parsed = response.json()
                    except ValueError:
                        parsed = text
                    if isinstance(parsed, dict):
                        raw = parsed
                        status_str = parsed.get("status", str(parsed))
                    else:
                        raw = parsed
                        status_str = str(parsed)
                
                # åˆæ­¥å°è£…ç»“æœ
                result_data = {
                    "dag_id": dag_id,
                    "status": status_str,
                    "is_running": status_str in ["running", "starting"],
                    # åé¢ä¼šæ ¹æ®ç›®å½•æ¥å£è¦†ç›– is_completed/is_failed
                    "is_completed": False,
                    "is_failed": False,
                    "raw_dag_response": raw,
                }
                
                # â€”â€” æ–°å¢ï¼šè°ƒç”¨ç»“æœç›®å½•æ¥å£æ ¡éªŒæœ€ç»ˆçŠ¶æ€ â€”â€” #
                try:
                    async with httpx.AsyncClient(timeout=30) as client:
                        catalog_resp = await client.get(
                            RESULT_CATALOG_URL,
                            params={"dagId": dag_id},
                            headers=final_headers
                        )
                    if catalog_resp.status_code == 200:
                        catalog = catalog_resp.json()
                        entries = catalog.get("data", [])
                        entry = next((e for e in entries if e.get("dagId") == dag_id), None)
                        if entry:
                            final_state = entry.get("state")
                            result_data["final_state"] = final_state
                            result_data["catalog_entry"] = entry
                            
                            if final_state == "success":
                                result_data["is_completed"] = True
                                result_data["is_failed"] = False
                            else:
                                result_data["is_completed"] = False
                                result_data["is_failed"] = True
                        else:
                            # æ²¡æœ‰æ‰¾åˆ°å¯¹åº”è®°å½•ï¼Œè§†ä¸ºå¼‚å¸¸
                            result_data["final_state"] = "unknown"
                            result_data["is_failed"] = True
                    else:
                        logger.warning(f"æŸ¥è¯¢ç»“æœç›®å½•å¤±è´¥ - HTTP {catalog_resp.status_code}")
                except Exception as e:
                    logger.error(f"æŸ¥è¯¢ç»“æœç›®å½•å¼‚å¸¸: {e}")
                # â€”â€” æ ¡éªŒç»“æŸ â€”â€” #
                
                result = Result.succ(
                    data=result_data,
                    msg=(
                        f"{operation}æˆåŠŸï¼ŒDAG çŠ¶æ€: {status_str}ï¼›"
                        f"æœ€ç»ˆç»“æœçŠ¶æ€: {result_data.get('final_state', 'unknown')}"
                    ),
                    operation=operation,
                    execution_time=execution_time,
                    api_endpoint="dag"
                )
                logger.info(
                    f"{operation}æˆåŠŸ - DAG ID: {dag_id}, "
                    f"æœ€ç»ˆç»“æœçŠ¶æ€: {result_data.get('final_state')}"
                )
            else:
                error_msg = f"HTTP {response.status_code}: {response.text}"
                result = Result.failed(
                    msg=f"{operation}å¤±è´¥: {error_msg}",
                    operation=operation
                )
                logger.error(f"{operation}å¤±è´¥ - {error_msg}")
        
        else:
            # ä½¿ç”¨å†…ç½‘ Tokenï¼ˆå¯åˆ·æ–°çš„é€»è¾‘ï¼‰
            get_params = {"dagId": dag_id}
            api_result, execution_time = await call_api_with_timing(
                url=f"{DAG_API_BASE_URL}/getState",
                method="GET",
                headers={"params": get_params},
                timeout=30,
                use_intranet_token=True
            )
            
            if "error" not in api_result:
                # status_str = api_result.get("status", str(api_result))
                if isinstance(api_result, dict):
                    status_str = api_result.get("status", str(api_result))
                else:
                    status_str = str(api_result)
                result_data = {
                    "dag_id": dag_id,
                    "status": status_str,
                    "is_running": status_str in ["running", "starting"],
                    "is_completed": False,
                    "is_failed": False,
                    "raw_dag_response": api_result,
                }
                
                # â€”â€” æ–°å¢ï¼šå†…ç½‘æ¨¡å¼ä¸‹è°ƒç”¨ç»“æœç›®å½•æ¥å£ â€”â€” #
                try:
                    catalog_result, _ = await call_api_with_timing(
                        url=RESULT_CATALOG_URL,
                        method="GET",
                        headers={"params": {"dagId": dag_id}},
                        timeout=30,
                        use_intranet_token=True
                    )
                    catalog_result, _ = await call_api_with_timing(
                    url=RESULT_CATALOG_URL,
                    method="GET",
                    headers={"params": {"dagId": dag_id}},
                    timeout=30,
                    use_intranet_token=True
                    )
                    # å…ˆå°è¯•æŠŠ catalog_result å˜æˆ dict
                    if isinstance(catalog_result, str):
                        try:
                            catalog_result = json.loads(catalog_result)
                        except json.JSONDecodeError:
                            catalog_result = None

                    if isinstance(catalog_result, dict) and "data" in catalog_result:
                        raw_list = catalog_result.get("data") or []
                        # åªä¿ç•™ dict é¡¹å¹¶åŒ¹é… dagId
                        entries = [e for e in raw_list if isinstance(e, dict)]
                        entry = next((e for e in entries if e.get("dagId") == dag_id), None)
                    if entry:
                        final_state = entry.get("state")
                        result_data["final_state"] = final_state
                        result_data["catalog_entry"] = entry
                        if final_state == "success":
                            result_data["is_completed"] = True
                            result_data["is_failed"] = False
                        else:
                            result_data["is_completed"] = False
                            result_data["is_failed"] = True
                    else:
                        result_data["final_state"] = "unknown"
                        result_data["is_failed"] = True
                except Exception as e:
                    logger.error(f"æŸ¥è¯¢ç»“æœç›®å½•å¼‚å¸¸: {e}")
                # â€”â€” æ ¡éªŒç»“æŸ â€”â€” #
                
                result = Result.succ(
                    data=result_data,
                    msg=(
                        f"{operation}æˆåŠŸï¼ŒDAG çŠ¶æ€: {status_str}ï¼›"
                        f"æœ€ç»ˆç»“æœçŠ¶æ€: {result_data.get('final_state', 'unknown')}"
                    ),
                    operation=operation,
                    execution_time=execution_time,
                    api_endpoint="dag"
                )
                logger.info(
                    f"{operation}æˆåŠŸ - DAG ID: {dag_id}, "
                    f"æœ€ç»ˆç»“æœçŠ¶æ€: {result_data.get('final_state')}"
                )
            else:
                result = Result.failed(
                    msg=f"{operation}å¤±è´¥: {api_result.get('error')}",
                    operation=operation
                )
                logger.error(f"{operation}å¤±è´¥ - {api_result.get('error')}")
        
        if ctx:
            await ctx.session.send_log_message(
                "info",
                f"{operation}æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶ {execution_time:.2f} ç§’"
            )
        return result.model_dump_json()
    
    except Exception as e:
        tb = traceback.format_exc()
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {e}\n{tb}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {e}\n{tb}",
            operation=operation
        )
        return result.model_dump_json()


# é€»è¾‘ç®€åŒ–ç‰ˆ
@mcp.tool()
async def query_task_status(
    dag_id: Annotated[str,Field(description="åœ°å—åˆå¹¶çš„processIdï¼Œä¹Ÿæ˜¯recordId/dagId",required = True)],
    auth_token: Annotated[str,Field(description="éªŒè¯tokenï¼Œä¸€èˆ¬ä¸éœ€è¦",required = False)] = None,
    ctx: Context = None
) -> str:
    """
    æŸ¥è¯¢æ‰¹å¤„ç†ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€ã€‚é™¤éç”¨æˆ·æŒ‡å®šä½¿ç”¨ï¼Œå¦åˆ™ä¸å»è°ƒç”¨ã€‚
    """
    operation = "æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"
    DAG_STATE_URL = f"{DAG_API_BASE_URL}/getState"
    # CATALOG_URL   = "http://172.20.70.141/api/asset/batch-result/catalog"

    # å‡†å¤‡ headers & æ¨¡å¼
    use_custom = bool(auth_token)
    if use_custom and not auth_token.startswith("Bearer "):
        auth_token = f"Bearer {auth_token}"
    common_headers = {"Content-Type": "application/json"} if use_custom else {}
    if use_custom:
        common_headers["Authorization"] = auth_token

    # å°å·¥å…·ï¼šå‘èµ·è¯·æ±‚ï¼ˆå¯å¤ç”¨ httpx æˆ– call_api_with_timingï¼‰
    async def fetch(url: str, params: dict):
        if use_custom:
            start = time.perf_counter()
            async with httpx.AsyncClient(timeout=30) as client:
                resp = await client.get(url, params=params, headers=common_headers)
            return resp, time.perf_counter() - start
        else:
            return await call_api_with_timing(
                url=url,
                method="GET",
                params=params,
                timeout=30,
                use_intranet_token=True
            )

    # å°å·¥å…·ï¼šè§£æ DAG æ¥å£è¿”å›ï¼Œç»Ÿä¸€æˆ (status_str, raw)
    def parse_status(resp_or_obj):
        # httpx.Response åˆ†æ”¯
        if isinstance(resp_or_obj, httpx.Response):
            text = resp_or_obj.text.strip()
            if not text:
                return "unknown", None
            try:
                parsed = resp_or_obj.json()
            except ValueError:
                return text, text
        else:
            parsed = resp_or_obj

        if isinstance(parsed, dict):
            return parsed.get("status", str(parsed)), parsed
        else:
            return str(parsed), parsed

    # å°å·¥å…·ï¼šæŸ¥è¯¢ç›®å½•å¹¶æå– entry
    async def check_catalog():
        cat_resp, _ = await fetch(CATALOG_URL, {"dagId": dag_id})
        # å¦‚æœæ˜¯ httpx.Responseï¼Œéœ€è¦å…ˆ .json()
        if isinstance(cat_resp, httpx.Response):
            if cat_resp.status_code != 200:
                logger.warning(f"æŸ¥è¯¢ç»“æœç›®å½•å¤±è´¥ HTTP {cat_resp.status_code}")
                return None
            try:
                catalog = cat_resp.json()
            except ValueError:
                return None
        else:
            catalog = cat_resp if isinstance(cat_resp, dict) else None

        if not isinstance(catalog, dict):
            return None

        data = catalog.get("data") or []
        for item in data:
            if isinstance(item, dict) and item.get("dagId") == dag_id:
                return item
        return None

    try:
        # if ctx:
        #     await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ {operation}...")
        logger.info(f"{operation} å¼€å§‹ - DAG ID: {dag_id}")

        result_data = {
            "dag_id": dag_id,
            "status": "",
            "is_running": "",
            "is_completed": False,
            "is_failed": False,
            "raw_dag_response": ""
        }


        # å…ˆç”¨ DAG API åˆ¤æ–­æ˜¯å¦è¿˜åœ¨è·‘
        raw_resp, elapsed = await fetch(DAG_STATE_URL, {"dagId": dag_id})
        status_str, raw = parse_status(raw_resp)
        result_data["status"]     = status_str
        result_data["is_running"] = status_str in ["starting","running"]
        # result_data["raw_dag_response"] = raw

        # åªæœ‰å½“ DAG ä¸å† running æ—¶ï¼Œæ‰å»æŸ¥ç›®å½•ç¡®è®¤æœ€ç»ˆç»“æœ
        if not result_data["is_running"]:
            entry = await check_catalog()
            if entry:
                final_state = entry.get("state")
                result_data["final_state"]   = final_state
                result_data["catalog_entry"] = entry
                result_data["is_completed"]  = (final_state == "success")
                result_data["is_failed"]     = (final_state in ["failed", "error", "dead", "killed"])
            else:
                result_data["final_state"] = "unknown"
                result_data["is_failed"]   = True
        else:
            # è¿˜åœ¨è·‘ï¼Œä¸åˆ¤å¤±è´¥
            result_data["final_state"] = None
            result_data["is_completed"] = False
            result_data["is_failed"]    = False

        # 4. æ„å»ºå¹¶è¿”å› Result
        result = Result.succ(
            data=result_data,
            msg=(
                f"{operation}æˆåŠŸï¼ŒDAG çŠ¶æ€: {status_str}ï¼›"
                f"æœ€ç»ˆç»“æœçŠ¶æ€: {result_data.get('final_state')}"
            ),
            operation=operation,
            map_type="query_task_status",
            execution_time=elapsed,
            api_endpoint="dag"
        )
        logger.info(f"{operation} æˆåŠŸ - æœ€ç»ˆçŠ¶æ€: {result_data.get('final_state')}")
        return result.model_dump_json()

    except Exception as e:
        tb = traceback.format_exc()
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {e}\n{tb}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {e}\n{tb}",
            map_type="query_task_status",
            operation=operation
        )
        return result.model_dump_json()


# @mcp.tool()
async def execute_dag_workflow(
    code: str,
    user_id: str = DEFAULT_USER_ID,
    sample_name: str = "",
    task_name: str = None,
    filename: str = None,
    crs: str = "EPSG:4326",
    scale: str = "1000",
    format: str = "tif",
    username: str = DEFAULT_USERNAME,
    auth_token: str = None,
    auto_submit: bool = True,
    wait_for_completion: bool = False,
    check_interval: int = 10,     # é»˜è®¤15ç§’æ£€æŸ¥ä¸€æ¬¡
    max_wait_time: int = 300,    # é»˜è®¤5åˆ†é’Ÿè¶…æ—¶
    ctx: Context = None
) -> str:
    """
    æ‰§è¡Œå®Œæ•´çš„DAGæ‰¹å¤„ç†å·¥ä½œæµï¼šä»£ç è½¬DAG -> æäº¤ä»»åŠ¡ -> (å¯é€‰)ç­‰å¾…å®Œæˆ
    
    Parameters:
    - code: OGEä»£ç 
    - user_id: ç”¨æˆ·UUID
    - sample_name: ç¤ºä¾‹ä»£ç åç§°
    - task_name: ä»»åŠ¡åç§°ï¼ˆå¯é€‰ï¼‰
    - filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
    - crs: åæ ‡å‚è€ƒç³»ç»Ÿ
    - scale: æ¯”ä¾‹å°º
    - format: è¾“å‡ºæ ¼å¼
    - username: ç”¨æˆ·å
    - auth_token: è®¤è¯Tokenï¼ˆå¯é€‰ï¼‰
    - auto_submit: æ˜¯å¦è‡ªåŠ¨æäº¤ä»»åŠ¡
    - wait_for_completion: æ˜¯å¦ç­‰å¾…ä»»åŠ¡å®Œæˆ
    - check_interval: çŠ¶æ€æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    - max_wait_time: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    """
    operation = "DAGæ‰¹å¤„ç†å·¥ä½œæµ"
    workflow_start_time = time.perf_counter()
    
    try:
        # if ctx:
        #     await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation}")
        
        workflow_results = {
            "steps": [],
            "final_status": "unknown",
            "dag_ids": [],
            "task_info": None,
            "execution_times": {}
        }
        
        # æ­¥éª¤1: ä»£ç è½¬DAG
        # if ctx:
        #     await ctx.session.send_log_message("info", "æ­¥éª¤1: ä»£ç è½¬æ¢ä¸ºDAG...")
        
        dag_result_json = await execute_code_to_dag(
            code=code,
            user_id=user_id,
            sample_name=sample_name,
            auth_token=auth_token,
            ctx=ctx
        )
        
        dag_result = json.loads(dag_result_json)
        workflow_results["steps"].append({
            "step": 1,
            "name": "ä»£ç è½¬DAG",
            "success": dag_result.get("success", False),
            "result": dag_result
        })
        
        if not dag_result.get("success"):
            workflow_results["final_status"] = "failed_at_dag_creation"
            result = Result.failed(
                msg=f"{operation}å¤±è´¥ï¼šä»£ç è½¬DAGæ­¥éª¤å¤±è´¥",
                map_type="execute_dag_workflow",
                operation=operation
            )
            result.data = workflow_results
            return result.model_dump_json()
        
        # è·å–DAGä¿¡æ¯
        dag_data = dag_result.get("data", {})
        dag_ids = dag_data.get("dag_ids", [])
        workflow_results["dag_ids"] = dag_ids
        
        if not dag_ids:
            workflow_results["final_status"] = "no_dag_generated"
            result = Result.failed(
                msg=f"{operation}å¤±è´¥ï¼šæœªç”ŸæˆDAGä»»åŠ¡",
                map_type="execute_dag_workflow",
                operation=operation
            )
            result.data = workflow_results
            return result.model_dump_json()
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªDAG ID
        primary_dag_id = dag_ids[0]
        logger.info(f"ä½¿ç”¨DAG ID: {primary_dag_id}")
        
        if auto_submit:
            # æ­¥éª¤2: æäº¤æ‰¹å¤„ç†ä»»åŠ¡
            # if ctx:
            #     await ctx.session.send_log_message("info", f"æ­¥éª¤2: æäº¤æ‰¹å¤„ç†ä»»åŠ¡ (DAG: {primary_dag_id})...")
            
            submit_result_json = await submit_batch_task(
                dag_id=primary_dag_id,
                task_name=task_name,
                filename=filename,
                crs=crs,
                scale=scale,
                format=format,
                username=username,
                script=code,
                auth_token=auth_token,
                ctx=ctx
            )
            
            # scriptå­—æ®µå†…å®¹å¤ªå¤šäº†ï¼Œæ˜¯æ‰§è¡Œçš„è„šæœ¬ï¼Œä¸éœ€è¦æš´éœ²å‡ºæ¥ã€‚
            submit_result = json.loads(submit_result_json)
            def _remove_script(obj):
                if isinstance(obj, dict):
                    obj.pop("script", None)
                    for v in obj.values():
                        _remove_script(v)
                elif isinstance(obj, list):
                    for item in obj:
                        _remove_script(item)

            _remove_script(submit_result)

            # tmp_data = submit_result.get("data")
            # if isinstance(tmp_data, dict) and "script" in tmp_data:
            #     tmp_data.pop("script", None)
            #     submit_result["data"] = tmp_data
            workflow_results["steps"].append({
                "step": 2,
                "name": "æäº¤æ‰¹å¤„ç†ä»»åŠ¡",
                "success": submit_result.get("success", False),
                "result": submit_result
            })
            
            if not submit_result.get("success"):
                workflow_results["final_status"] = "failed_at_task_submission"
                result = Result.failed(
                    msg=f"{operation}å¤±è´¥ï¼šä»»åŠ¡æäº¤æ­¥éª¤å¤±è´¥",
                    map_type="execute_dag_workflow",
                    operation=operation
                )
                result.data = workflow_results
                return result.model_dump_json()
            
            # è·å–ä»»åŠ¡ä¿¡æ¯
            task_data = submit_result.get("data", {})
            workflow_results["task_info"] = task_data
            
            if wait_for_completion:
                # æ­¥éª¤3: ç­‰å¾…ä»»åŠ¡å®Œæˆ
                # if ctx:
                #     await ctx.session.send_log_message("info", f"æ­¥éª¤3: ç­‰å¾…ä»»åŠ¡å®Œæˆ...")
                # ç­‰å¾…6sï¼Œç­‰ä»»åŠ¡çœŸçš„æäº¤åå†å»æŸ¥è¯¢
                await asyncio.sleep(10)
                
                waited_time = 0
                final_status = "unknown"
                try:
                    while waited_time < max_wait_time:
                        status_result_json = await query_task_status(
                            dag_id=primary_dag_id,
                            # auth_token=auth_token,
                            ctx=None  # é¿å…è¿‡å¤šæ—¥å¿—
                        )
                        
                        status_result = json.loads(status_result_json)
                        
                        if status_result.get("success"):
                            status_data = status_result.get("data", {})
                            current_status = status_data.get("status", "unknown")
                            
                            if status_data.get("is_completed"):
                                final_status = "completed"
                                workflow_results["final_status"] = "completed"
                                logger.info(f"ä»»åŠ¡å·²å®Œæˆ: {current_status}")
                                break
                            elif status_data.get("is_failed"):
                                final_status = "failed"
                                workflow_results["final_status"] = "failed"
                                logger.info(f"ä»»åŠ¡å¤±è´¥: {current_status}")
                                break
                            # else:
                            #     # ä»»åŠ¡ä»åœ¨è¿è¡Œ
                            #     if ctx:
                            #         await ctx.session.send_log_message("info", f"ä»»åŠ¡çŠ¶æ€: {current_status}, å·²ç­‰å¾… {waited_time}s")
                        
                        await asyncio.sleep(check_interval)
                        waited_time += check_interval
                except Exception as e:
                    tb = traceback.format_exc()
                    logger.error(f"query_task_status æŠ¥é”™ï¼š{tb}", exc_info=True)
                    print(tb)
                if waited_time >= max_wait_time:
                    workflow_results["final_status"] = "timeout"
                    final_status = "timeout"
                
                workflow_results["steps"].append({
                    "step": 3,
                    "name": "ç­‰å¾…ä»»åŠ¡å®Œæˆ",
                    "success": final_status == "completed",
                    "final_status": final_status,
                    "waited_time": waited_time
                })
            else:
                workflow_results["final_status"] = "submitted"
        else:
            workflow_results["final_status"] = "dag_created"
        
        total_execution_time = time.perf_counter() - workflow_start_time
        workflow_results["execution_times"]["total"] = total_execution_time
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        if workflow_results["final_status"] in ["completed", "submitted", "dag_created"]:
            result = Result.succ(
                data=workflow_results,
                msg=f"{operation}æˆåŠŸï¼ŒçŠ¶æ€: {workflow_results['final_status']}",
                map_type="execute_dag_workflow",
                operation=operation,
                execution_time=total_execution_time,
                api_endpoint="dag"
            )
        else:
            result = Result.failed(
                msg=f"{operation}å®Œæˆä½†çŠ¶æ€å¼‚å¸¸: {workflow_results['final_status']}",
                map_type="execute_dag_workflow",
                operation=operation
            )
            result.data = workflow_results
        
        # if ctx:
        #     await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶{total_execution_time:.2f}ç§’")
        
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            map_type="execute_dag_workflow",
            operation=operation
        )
        result.data = workflow_results
        return result.model_dump_json()

# ============ å…¶ä»–æ–¹æ³• ============

def update_process_id(data: dict, new_process_id: str):
    """ç”¨äº630æ¼”ç¤ºï¼Œæ›´æ–°processId"""
    if "aft" in data:
        for item in data["aft"]:
            item["processId"] = new_process_id
    return data


# ============ HTTPæœåŠ¡å™¨è®¾ç½® ============

def create_starlette_app(mcp_server: Server, *, debug: bool = False) -> Starlette:
    """åˆ›å»ºæ”¯æŒSSEçš„Starletteåº”ç”¨"""
    sse = SseServerTransport("/messages/")

    async def handle_sse(request: Request) -> None:
        async with sse.connect_sse(
            request.scope,
            request.receive,
            request._send,
        ) as (read_stream, write_stream):
            await mcp_server.run(
                read_stream,
                write_stream,
                mcp_server.create_initialization_options(),
            )

    async def handle_health(request: Request):
        return JSONResponse({
            "status": "healthy",
            "server": MCP_SERVER_NAME,
            "endpoints": {
                "sse": "/sse",
                "health": "/health",
                "messages": "/messages/"
            }
        })

    async def handle_info(request: Request):
        return JSONResponse({
            "server_name": MCP_SERVER_NAME,
            "version": "2.4.0",
            "description": "å±±ä¸œè€•åœ°æµå‡ºåˆ†æMCPæœåŠ¡å™¨ - å¢å¼ºç‰ˆ (10ä¸ªæ ¸å¿ƒå·¥å…· + è‡ªåŠ¨Tokenç®¡ç†)",
            "features": [
                "è‡ªåŠ¨Tokenåˆ·æ–°",
                "æ‰‹åŠ¨Tokenåˆ·æ–°",
                "å¡å‘åˆ†æ", 
                "å±±ä¸œè€•åœ°æµå‡ºåˆ†æ",
                "å¤šçº¦æŸè€•åœ°æµå‡ºåˆ†æ",
                "å¤§æ•°æ®æŸ¥è¯¢",
                "DAGæ‰¹å¤„ç†å·¥ä½œæµ",
                "ä»£ç è½¬DAGä»»åŠ¡",
                "æ‰¹å¤„ç†ä»»åŠ¡æäº¤",
                "ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢",
                "SSEä¼ è¾“",
                "HTTP endpoints",
                "ç»“æ„åŒ–æ—¥å¿—",
                "æ€§èƒ½ç›‘æ§"
            ],
            "apis": {
                "intranet_api": INTRANET_API_BASE_URL,
                "dag_api": DAG_API_BASE_URL
            },
            "available_tools": [
                "refresh_token",
                "check_token_status",
                "coverage_aspect_analysis", 
                "shandong_farmland_outflow",
                "farmland_outflow",
                "run_big_query",
                "execute_code_to_dag",
                "submit_batch_task", 
                "query_task_status",
                "execute_dag_workflow"
            ],
            "token_management": {
                "type": "automatic",
                "description": "è‡ªåŠ¨æ£€æµ‹tokenè¿‡æœŸ(40003)å¹¶åˆ·æ–°ï¼Œä¹Ÿæ”¯æŒæ‰‹åŠ¨åˆ·æ–°",
                "auto_refresh": "æ£€æµ‹åˆ°40003é”™è¯¯æ—¶è‡ªåŠ¨åˆ·æ–°",
                "manual_refresh": "å¯ä½¿ç”¨refresh_tokenå·¥å…·æ‰‹åŠ¨åˆ·æ–°", 
                "credentials": "edu_admin/123456",
                "format": "Bearer <jwt_token>"
            }
        })

    return Starlette(
        debug=debug,
        routes=[
            Route("/sse", endpoint=handle_sse),
            Route("/health", endpoint=handle_health),
            Route("/info", endpoint=handle_info),
            Mount("/messages/", app=sse.handle_post_message),
        ],
    )

# ============ ä¸»ç¨‹åº ============

async def run_stdio_server():
    """è¿è¡Œstdioæ¨¡å¼çš„æœåŠ¡å™¨"""
    logger.info("å¯åŠ¨å±±ä¸œè€•åœ°æµå‡ºåˆ†æMCPæœåŠ¡å™¨ (stdioæ¨¡å¼)...")
    
    try:
        from mcp import stdio_server
        
        async with stdio_server() as streams:
            await mcp._mcp_server.run(
                streams[0], streams[1], 
                mcp._mcp_server.create_initialization_options()
            )
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡å™¨...")
    except Exception as e:
        logger.error(f"æœåŠ¡å™¨è¿è¡Œå‡ºé”™: {e}")
    finally:
        logger.info("MCPæœåŠ¡å™¨å·²å…³é—­")

def run_http_server(host: str = "0.0.0.0", port: int = 8000):
    """è¿è¡ŒHTTPæ¨¡å¼çš„æœåŠ¡å™¨"""
    logger.info(f"å¯åŠ¨å±±ä¸œè€•åœ°æµå‡ºåˆ†æMCPæœåŠ¡å™¨ (HTTPæ¨¡å¼) - {host}:{port}")
    
    mcp_server = mcp._mcp_server
    starlette_app = create_starlette_app(mcp_server, debug=True)
    
    uvicorn.run(starlette_app, host=host, port=port)

# åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ æµ‹è¯•å·¥å…·

# @mcp.tool()
async def test_dag_status_api(
    dag_id: str,
    ctx: Context = None
) -> str:
    """
    æµ‹è¯•DAGçŠ¶æ€æŸ¥è¯¢API - ç›´æ¥è°ƒç”¨ä¸ç»è¿‡å°è£…
    
    ç”¨äºè¯Šæ–­query_task_statusçš„é—®é¢˜
    """
    operation = "æµ‹è¯•DAGçŠ¶æ€API"
    
    try:
        if ctx:
            await ctx.session.send_log_message("info", f"å¼€å§‹æ‰§è¡Œ{operation}...")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œ{operation} - DAG ID: {dag_id}")
        
        # æ„å»ºAPI URL
        api_url = f"{DAG_API_BASE_URL}/getState"
        params = {"dagId": dag_id}
        
        logger.info(f"æµ‹è¯•APIè°ƒç”¨: {api_url}?dagId={dag_id}")
        
        start_time = time.perf_counter()
        
        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.get(
                api_url,
                params=params,
                headers={
                    "Content-Type": "application/json",
                    "Authorization": INTRANET_AUTH_TOKEN
                }
            )
            
            execution_time = time.perf_counter() - start_time
            
            # è¯¦ç»†è®°å½•å“åº”ä¿¡æ¯
            response_info = {
                "status_code": response.status_code,
                "headers": dict(response.headers),
                "content_length": len(response.content),
                "text_preview": response.text[:200] if response.text else "Empty",
                "is_json": False,
                "execution_time": execution_time
            }
            
            # å°è¯•è§£æJSON
            json_data = None
            try:
                json_data = response.json()
                response_info["is_json"] = True
                response_info["json_data"] = json_data
            except Exception as e:
                response_info["json_error"] = str(e)
            
            result = Result.succ(
                data=response_info,
                msg=f"{operation}å®Œæˆ - çŠ¶æ€ç : {response.status_code}",
                map_type="test_dag_status_api",
                operation=operation,
                execution_time=execution_time,
                api_endpoint="dag_test"
            )
            
            logger.info(f"{operation}å®Œæˆ - çŠ¶æ€ç : {response.status_code}, å†…å®¹é•¿åº¦: {len(response.content)}")
            
        if ctx:
            await ctx.session.send_log_message("info", f"{operation}æ‰§è¡Œå®Œæˆ")
        
        return result.model_dump_json()
        
    except Exception as e:
        logger.error(f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}")
        result = Result.failed(
            msg=f"{operation}æ‰§è¡Œå¤±è´¥: {str(e)}",
            map_type="test_dag_status_api",
            operation=operation
        )
        return result.model_dump_json()
    
# æœ¬åœ°æµ‹è¯•ç”¨
# if __name__ == "__main__":
#     # asyncio.run(shandong_farmland_vector_query_new(["ç‹‚å±±æ‘","é˜¿è¨å¾·è§å®¢æˆ·","é›ªé‡é•‡"]))
#     asyncio.run(farmland_suitability_analysis("SELECT * FROM shp_guotubiangeng WHERE DLMC IN ('æ—±åœ°', 'æ°´æµ‡åœ°', 'æ°´ç”°') AND ZLDWMC IN ('ç‹‚å±±æ‘', 'ä¸œä¸‹æ¸¸æ‘')"))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='å±±ä¸œè€•åœ°æµå‡ºåˆ†æMCPæœåŠ¡å™¨ - å¢å¼ºç‰ˆ')
    parser.add_argument('--mode', choices=['stdio', 'http'], default='stdio', help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--host', default='0.0.0.0', help='HTTPæ¨¡å¼çš„ç»‘å®šåœ°å€')
    parser.add_argument('--port', type=int, default=8000, help='HTTPæ¨¡å¼çš„ç›‘å¬ç«¯å£')
    
    args = parser.parse_args()
    
    try:
        if args.mode == 'stdio':
            asyncio.run(run_stdio_server())
        else:
            run_http_server(args.host, args.port)
    except KeyboardInterrupt:
        print("\næœåŠ¡å™¨å·²åœæ­¢")
    except Exception as e:
        logger.error(f"å¯åŠ¨å¤±è´¥: {e}") 

